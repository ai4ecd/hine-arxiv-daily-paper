# arxiv-daily latest papers around hine arxiv daily paper
Automated deployment @ 2023-06-10 17:00:45 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`]({repo_url}/blob/main/database/topic.yml).
> You can also view historical data through the [storage]({repo_url}/blob/main/database/storage).

## HINE

### hine
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**RDumb: A simple approach that questions our progress in continual test-time adaptation**|Ori Press et.al.|[2306.05401v1](http://arxiv.org/abs/2306.05401v1)|[link](https://github.com/oripress/ccc)|Test-Time Adaptation (TTA) allows to update pretrained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continuously Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, "RDumb", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing adaptation to avoid collapse nor able to outperform a simplistic resetting strategy.|
|**2023-06-08**|**Research Impact of Solar Panel Cleaning Robot on Photovoltaic Panel's Deflection**|Trung Dat Phan et.al.|[2306.05340v1](http://arxiv.org/abs/2306.05340v1)|null|In the last few decades, solar panel cleaning robots (SPCR) have been widely used for sanitizing photovoltaic (PV) panels as an effective solution for ensuring PV efficiency. However, the dynamic load generated by the SPCR during operation might have a negative impact on PV panels. To reduce these effects, this paper presents the utilization of ANSYS software to simulate multiple scenarios involving the impact of SPCR on PV panels. The simulation scenarios provided in the paper are derived from the typical movements of SPCR observed during practical operations. The simulation results show the deformation process of PV panels, and a second-order polynomial is established to describe the deformed amplitude along the centerline of PV panels. This second-order polynomial contributes to the design process of a damper system for SPCR aiming to reduce the influence of SPCR on PV panels. Moreover, the experiments are conducted to examine the correlation between the results of the simulation and the experiment.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our solver relies on a relaxation method in which convergence is sought as the steady-state solution of a parabolic equation, whose time-discretization is governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings indicate that the RKL-based Poisson solver, which is both fully parallel and rapidly convergent, has the potential to serve as a practical alternative to conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and \textit{successive over-relaxation} (SOR) methods. Additionally, it can mitigate some of the drawbacks of these traditional techniques. We incorporate our algorithm into a multigrid solver to provide a simple and efficient gravity solver that can be used to obtain the gravitational potentials in self-gravitational hydrodynamics. We test our implementation against a broad range of standard self-gravitating astrophysical problems designed to examine different aspects of the code. We demonstrate that the results match excellently with the analytical predictions (when available), and the findings of similar previous studies.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation. The robot decides whether to proceed by examining the tracker's bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker's ability to track target dishes and to determine the exit status. Our tracker shows 38% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Analysis of Knuth's Sampling Algorithm D and D'**|Mridul Nandi et.al.|[2306.05243v1](http://arxiv.org/abs/2306.05243v1)|null|In this research paper, we address the Distinct Elements estimation problem in the context of streaming algorithms. The problem involves estimating the number of distinct elements in a given data stream $\mathcal{A} = (a_1, a_2,\ldots, a_m)$, where $a_i \in \{1, 2, \ldots, n\}$. Over the past four decades, the Distinct Elements problem has received considerable attention, theoretically and empirically, leading to the development of space-optimal algorithms. A recent sampling-based algorithm proposed by Chakraborty et al.[11] has garnered significant interest and has even attracted the attention of renowned computer scientist Donald E. Knuth, who wrote an article on the same topic [6] and called the algorithm CVM. In this paper, we thoroughly examine the algorithms (referred to as CVM1, CVM2 in [6] and DonD, DonD' in [6]. We first unify all these algorithms and call them cutoff-based algorithms. Then we provide an approximation and biasedness analysis of these algorithms.|
|**2023-06-08**|**Design of Sturm global attractors 2: Time-reversible Chafee-Infante lattices of 3-nose meanders**|Bernold Fiedler et.al.|[2306.05232v1](http://arxiv.org/abs/2306.05232v1)|null|This sequel continues our exploration arxiv:2302.12531 of a deceptively ``simple'' class of global attractors, called Sturm due to nodal properties. They arise for the semilinear scalar parabolic PDE   \begin{equation}\label{eq:*}   u_t = u_{xx} + f(x,u,u_x) \tag{$*$}   \end{equation} on the unit interval $0 < x<1$, under Neumann boundary conditions. This models the interplay of reaction, advection, and diffusion.   Our classification is based on the Sturm meanders, which arise from a shooting approach to the ODE boundary value problem of equilibrium solutions $u=v(x)$. Specifically, we address meanders with only three ``noses'', each of which is innermost to a nested family of upper or lower meander arcs. The Chafee-Infante paradigm of 1974, with cubic nonlinearity $f=f(u)$, features just two noses.   We present, and fully prove, a precise description of global PDE connection graphs, graded by Morse index, for such gradient-like Morse-Smale systems \eqref{eq:*}. The directed edges denote PDE heteroclinic orbits $v_1 \leadsto v_2$ between equilibrium vertices $v_1, v_2$ of adjacent Morse index. The connection graphs can be described as a lattice-like structure of Chafee-Infante subgraphs. However, this simple description requires us to adjoin a single ``equilibrium'' vertex, formally, at Morse level -1. Surprisingly, for parabolic PDEs based on irreversible diffusion, the connection graphs then also exhibit global time reversibility.|
|**2023-06-08**|**Prospects for charged Higgs bosons in natural SUSY models at the high-luminosity LHC**|Howard Baer et.al.|[2306.05207v1](http://arxiv.org/abs/2306.05207v1)|null|We continue our examination of prospects for discovery of heavy Higgs bosons of natural SUSY (natSUSY) models at the high luminosity LHC (HL-LHC), this time focussing on charged Higgs bosons. In natSUSY, higgsinos are expected at the few hundred GeV scale whilst electroweak gauginos inhabit the TeV scale and the heavy Higgs bosons, H, A and H^\pm could range up tens of TeV without jeopardizing naturalness. For TeV-scale heavy SUSY Higgs bosons H, A and H^\pm, as currently required by LHC searches, SUSY decays into gaugino plus higgsino can dominate H^\pm decays provided these decays are kinematically accessible. The visible decay products of higgsinos are soft making them largely invisible, whilst the gauginos decay to W, Z or h plus missing transverse energy (MET). Charged Higgs bosons are dominantly produced at LHC14 via the parton subprocess, gb-> H^\pm t. In this paper, we examine the viability of observing signtures from H^\pm -> \tau\nu, H^\pm -> tb and H^\pm -> W, Z, h + MET events produced in association with a top quark at the HL-LHC over large Standard Model (SM) backgrounds from (mainly) t\bar{t}, t\bar{t}V and t\bar{t}h production (where V=W, Z). We find that the greatest reach is found via the SM H^\pm(-> \tau\nu) +t channel with a subdominant contribution from the H^\pm(-> tb) +t channel. Unlike for neutral Higgs searches, the SUSY decay modes appear to be unimportant for H^\pm searches at the HL-LHC. We delineate regions of the m_A vs. \tan\beta plane, mostly around m_A \sim 1-2 TeV, where signals from charged Higgs bosons would serve to confirm signals of a heavy, neutral Higgs boson at the 5\sigma level or, alternatively, to exclude heavy Higgs bosons at the 95% confidence level at the high luminosity LHC.|
|**2023-06-08**|**M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models**|Wenxuan Zhang et.al.|[2306.05179v1](http://arxiv.org/abs/2306.05179v1)|[link](https://github.com/damo-nlp-sg/m3exam)|Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.|
|**2023-06-08**|**Characterization of Multi-Channel Denial-of-Service and Full-Scale Denial-of-Service**|Anindya Basu et.al.|[2306.05161v1](http://arxiv.org/abs/2306.05161v1)|null|Over the past decades, interest in enhancing the safety of cyber-physical systems (CPSs) has risen. The systems and control research society has recognised that the embedded closed-loop in integrated systems may be damaged if attackers can execute a successful malicious attack. This article examines the resilient control problem for CPSs with numerous transmission channels under Denial-of-Service (DoS). First, a partial observer technique is developed in response to the Multi-Channel DoS (MCDoS) condition. The changing frequency of MCDoS is characterized while maintaining the Global Asymptotic Stability (GAS) of the closed loop system. The partial observer is modified then to reduce the effect of the changing frequency of MCDoS in the system. Then a resilient event-based feedback control scheme is developed to address the Full-Scale DoS (FSDoS). We depict the changing frequency of MCDoS and the frequency and duration of FSDoS, allowing the feedback system's Global Asymptotic Stability (GAS) to be maintained. We regard event-based controllers for which a minimal inter-sample time is precisely formulated in response to the existence of digital channels.|
|**2023-06-08**|**Electronic correlations and superconducting instability in La$_3$Ni$_2$O$_7$ under high pressure**|Frank Lechermann et.al.|[2306.05121v1](http://arxiv.org/abs/2306.05121v1)|null|Motivated by the report of superconductivity in bilayer La$_3$Ni$_2$O$_7$ at high pressure, we examine the interacting electrons in this system. First-principles many-body theory is utilized to study the normal-state electronic properties. Below 100\,K, a multi-orbital non-Fermi liquid state resulting from loss of Ni-ligand coherence within a flat-band dominated low-energy landscape is uncovered. The incoherent low-temperature Fermi surface displays strong mixing between Ni-$d_{z^2}$ and Ni-$d_{x^2-y^2}$ orbital character. In a model-Hamiltonian picture, spin fluctuations originating mostly from the Ni-$d_{z^2}$ orbital give rise to strong tendencies towards a superconducting instability with $d_{x^2-y^2}$ order parameter. The dramatic enhancement of $T_{\rm c}$ in pressurized La$_3$Ni$_2$O$_7$ is due to stronger Ni-$d_{z^2}$ correlations compared to those in the infinite-layer nickelates.|
|**2023-06-08**|**Stabilization of approximate GHZ state with quasi-local couplings**|Vincent Martin et.al.|[2306.05070v1](http://arxiv.org/abs/2306.05070v1)|null|We propose a reservoir design, composed of fixed dissipation operators acting each on few local subsystems, to stabilize an approximate GHZ state on n qubits. The main idea is to work out how a previously proposed sequence of two stabilization steps can be applied instead in appropriate (probabilistic) superposition. We examine alternatives to synchronize the superposition using local couplings only, thanks to a chain of "clock" ancillas or to additional levels on the data subsystems. The practical value of these alternatives depends on experimental constraints. They all feature a design tradeoff between approximate stabilization fidelity and protection against perturbations. These proposals illustrate how simple autonomous automata can be implemented in quantum reservoir engineering to replace sequential state preparation procedures. Encoding automaton actions via additional data levels only, appears particularly efficient in this context. Our analysis method, reducing the Lindblad master equation to a Markov chain on virtual output signals, may be of independent interest.|
|**2023-06-08**|**Leveraging Language Identification to Enhance Code-Mixed Text Classification**|Gauri Takawane et.al.|[2306.04964v1](http://arxiv.org/abs/2306.04964v1)|null|The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comparing their results with and without using word-level language information. The models were evaluated using metrics such as accuracy, precision, recall, and F1 score. Our findings show that the proposed language augmentation approaches work well across different BERT models. We demonstrate the importance of augmenting code-mixed text with language information on five different code-mixed Hindi-English downstream datasets based on sentiment analysis, hate speech detection, and emotion detection.|
|**2023-06-08**|**Towards a Success Model for Automated Programming Assessment Systems Used as a Formative Assessment Tool**|Clemens Sauerwein et.al.|[2306.04958v1](http://arxiv.org/abs/2306.04958v1)|null|The assessment of source code in university education is a central and important task for lecturers of programming courses. In doing so, educators are confronted with growing numbers of students having increasingly diverse prerequisites, a shortage of tutors, and highly dynamic learning objectives. To support lecturers in meeting these challenges, the use of automated programming assessment systems (APASs), facilitating formative assessments by providing timely, objective feedback, is a promising solution. Measuring the effectiveness and success of these platforms is crucial to understanding how such platforms should be designed, implemented, and used. However, research and practice lack a common understanding of aspects influencing the success of APASs. To address these issues, we have devised a success model for APASs based on established models from information systems as well as blended learning research and conducted an online survey with 414 students using the same APAS. In addition, we examined the role of mediators intervening between technology-, system- or self-related factors, respectively, and the users' satisfaction with APASs. Ultimately, our research has yielded a model of success comprising seven constructs influencing user satisfaction with an APAS.|
|**2023-06-08**|**Robust Learning with Progressive Data Expansion Against Spurious Correlation**|Yihe Deng et.al.|[2306.04949v1](http://arxiv.org/abs/2306.04949v1)|null|While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as ResNets and Transformers. On average, our method achieves a 2.8% improvement in worst-group accuracy compared with the state-of-the-art method, while enjoying up to 10x faster training efficiency.|
|**2023-06-08**|**Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes**|Peizhong Ju et.al.|[2306.04901v1](http://arxiv.org/abs/2306.04901v1)|null|Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the number of features/parameters in both underparameterized and overparameterized regimes. Furthermore, we provide practical guidelines for determining the number of features in the common and task-specific parts for improved generalization performance. For example, when the total number of features in the source task's learning model is fixed, we show that it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part. Moreover, in specific scenarios, particularly those characterized by high noise levels and small true parameters, sacrificing certain true features in the common part in favor of employing more redundant features in the task-specific part can yield notable benefits.|
|**2023-06-08**|**Dear Magellanic Clouds, welcome back!**|Eugene Vasiliev et.al.|[2306.04837v1](http://arxiv.org/abs/2306.04837v1)|null|We propose a scenario in which the Large Magellanic Cloud (LMC) is on its second passage around the Milky Way. Using a series of tailored N-body simulations, we demonstrate that such orbits are consistent with current observational constraints on the mass distribution and relative velocity of both galaxies. The previous pericentre passage of the LMC could have occurred 5-10 Gyr ago at a distance >~100 kpc, large enough to retain its current population of satellites. The perturbations of the Milky Way halo induced by the LMC look nearly identical to the first-passage scenario, however, the distribution of LMC debris is considerably broader in the second-passage model. We examine the likelihood of current and past association with the Magellanic system for dwarf galaxies in the Local Group, and find that in addition to 10-11 current LMC satellites, it could have brought a further 4-6 galaxies that have been lost after the first pericentre passage. In particular, four of the classical dwarfs - Carina, Draco, Fornax and Ursa Minor - each have a ~50% probability of once belonging to the Magellanic system, thus providing a possible explanation for the ``plane of satellites'' conundrum.|
|**2023-06-07**|**Perspectives in closed-loop supply chains network design considering risk and uncertainty factors**|Yang Hu et.al.|[2306.04819v1](http://arxiv.org/abs/2306.04819v1)|null|Risk and uncertainty in each stage of CLSC have greatly increased the complexity and reduced process efficiency of the closed-loop networks, impeding the sustainable and resilient development of industries and the circular economy. Recently, increasing interest in academia have been raised on the risk and uncertainty analysis of closed-loop supply chain, yet there is no comprehensive review paper focusing on closed-loop network design considering risk and uncertainty. This paper examines previous research on the domain of closed-loop network design under risk and uncertainties to provide constructive prospects for future study. We selected 106 papers published in the Scopus database from the year 2004 to 2022. We analyse the source of risk and uncertainties of the CLSC network and identified appropriate methods for handling uncertainties in addition to algorithms for solving uncertain CLSCND problems. We also illustrate the evolution of objectives for designing a closed-loop supply chain that is expos to risk or uncertainty, and investigate the application of uncertain network design models in practical industry sectors. Finally, we draw proper research gaps for each category and clarify some novel insights for future study. By considering the impacts of risk or uncertainties of different sources on closed-loop supply chain network design, we can approach the economical, sustainable, social, and resilient objectives effectively and efficiently.|
|**2023-06-07**|**Compressibility and speeds of sound across the superfluid to supersolid phase transition of an elongated dipolar gas**|P. B. Blakie et.al.|[2306.04794v1](http://arxiv.org/abs/2306.04794v1)|null|We investigate the excitation spectrum and compressibility of a dipolar Bose-Einstein condensate in an infinite tube potential in the parameter regime where the transition between superfluid and supersolid phases occurs. Our study focuses on the density range in which crystalline order develops continuously across the transition. Above the transition the superfluid shows a single gapless excitation band, phononic at small momenta and with a roton at a finite momentum. Below the transition, two gapless excitations branches (three at the transition point) emerge in the supersolid. We examine the two gapless excitation bands and their associated speeds of sound in the supersolid phase. Our results show that the speeds of sound and the compressibility are discontinuous at the transition, indicating a second-order phase transition. These results provide valuable insights into the identification of supersolid phenomena in dipolar quantum gases and the relationship to supersolidity in spin-orbit coupled gases.|
|**2023-06-07**|**The Temperature, Electron, and Pressure Characteristics of Switchbacks: Parker Solar Probe Observations**|Jia Huang et.al.|[2306.04773v1](http://arxiv.org/abs/2306.04773v1)|null|Parker Solar Probe (PSP) observes unexpectedly prevalent switchbacks, which are rapid magnetic field reversals that last from seconds to hours, in the inner heliosphere, posing new challenges to understanding their nature, origin, and evolution. In this work, we investigate the thermal states, electron pitch angle distributions, and pressure signatures of both inside and outside switchbacks, separating a switchback into spike, transition region (TR), and quiet period (QP). Based on our analysis, we find that the proton temperature anisotropies in TRs seem to show an intermediate state between spike and QP plasmas. The proton temperatures are more enhanced in spike than in TR and QP, but the alpha temperatures and alpha-to-proton temperature ratios show the opposite trends, implying that the preferential heating mechanisms of protons and alphas are competing in different regions of switchbacks. Moreover, our results suggest that the electron integrated intensities are almost the same across the switchbacks but the electron pitch angle distributions are more isotropic inside than outside switchbacks, implying switchbacks are intact structures but strong scattering of electrons happens inside switchbacks. In addition, the examination of pressures reveals that the total pressures are comparable through a switchback, confirming switchbacks are pressure-balanced structures. These characteristics could further our understanding of ion heating, electron scattering, and the structure of switchbacks.|
|**2023-06-07**|**Automatic retrieval of corresponding US views in longitudinal examinations**|Hamideh Kerdegari et.al.|[2306.04739v1](http://arxiv.org/abs/2306.04739v1)|null|Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%+/-0.24% error in cross-sectional area. Furthermore, a user study survey confirmed the efficacy of our model for muscle view retrieval.|
|**2023-06-07**|**Effects of Pressure on the Electronic and Magnetic Properties of Bulk NiI$_{2}$**|Jesse Kapeghian et.al.|[2306.04729v1](http://arxiv.org/abs/2306.04729v1)|null|Transition metal dihalides have recently garnered interest in the context of two-dimensional van der Waals magnets as their underlying geometrically frustrated triangular lattice leads to interesting competing exchange interactions. In particular, NiI$_{2}$ is a magnetic semiconductor that has been long known for its exotic helimagnetism in the bulk. Recent experiments have shown that the helimagnetic state survives down to the monolayer limit with a layer-dependent magnetic transition temperature that suggests a relevant role of the interlayer coupling. Here, we explore the effects of hydrostatic pressure as a means to enhance this interlayer exchange and ultimately tune the electronic and magnetic response of NiI$_{2}$. We study first the evolution of the structural parameters as a function of external pressure using first-principles calculations combined with x-ray diffraction measurements. We then examine the evolution of the electronic structure and magnetic exchange interactions via first-principles calculations and Monte Carlo simulations. We find that the leading interlayer coupling is an antiferromagnetic second-nearest neighbor interaction that increases monotonically with pressure. The ratio between isotropic third- and first-nearest neighbor intralayer exchanges, which controls the magnetic frustration and determines the magnetic propagation vector $\mathbf{q}$ of the helimagnetic ground state, is also enhanced by pressure. As a consequence, our Monte Carlo simulations show a monotonic increase in the magnetic transition temperature, indicating that pressure is an effective means to tune the magnetic response of NiI$_{2}$.|
|**2023-06-07**|**A New Family of Regression Models for $[0,1]$ Outcome Data: Expanding the Palette**|Eugene D. Hahn et.al.|[2306.04708v1](http://arxiv.org/abs/2306.04708v1)|null|Beta regression is a popular methodology when the outcome variable $y$ is on the open interval $(0,1)$. When $y$ is in the closed interval $[0,1]$, it is commonly accepted that beta regression is inapplicable. Instead, common solutions are to use augmented beta regression or censoring models or else to subjectively rescale the endpoints to allow beta regression. We provide an attractive new approach with a family of models that treats the entirety of $y\in[0,1]$ in a single model without rescaling or the need for the complications of augmentation or censoring. This family provides the interpretational convenience of a single straightforward model for the expectation of $y \in [0,1]$ over its entirety. We establish the conditions for the existence of a unique MLE and then examine this new family of models from both maximum-likelihood and Bayesian perspectives. We successfully apply the models to employment data in which augmented beta regression was difficult due to data separation. We also apply the models to healthcare panel data that were originally examined by way of rescaling.|
|**2023-06-07**|**Shadow and deflection angle of asymptotic, magnetically-charged, non-singular black hole**|Yashmitha Kumaran et.al.|[2306.04705v1](http://arxiv.org/abs/2306.04705v1)|null|In this paper, we present a detailed analysis of an asymptotic, magnetically-charged, non-singular (AMCNS) black hole. By utilizing the Gauss-bonnet theorem, we aim to unravel the intricate astrophysics associated with this unique black hole. The study explored various aspects including the black hole's gravitational field, intrinsic properties, light bending, the shadow and greybody bounding of the black hole. Through rigorous calculations and simulations, we derive the weak deflection angle of the optical metric of AMCNS black hole. Additionally, we investigate the impact of the dark matter medium on the deflection angle, examined the distinctive features of the black hole's shadow, and bound its greybody factors. Our findings not only deepen our understanding of gravitational lensing but also pave the way for future improvements in black hole theories by minimizing restrictive assumptions and incorporating a more realistic representation of these cosmic phenomena.|
|**2023-06-07**|**Tree-Regularized Bayesian Latent Class Analysis for Improving Weakly Separated Dietary Pattern Subtyping in Small-Sized Subpopulations**|Mengbing Li et.al.|[2306.04700v1](http://arxiv.org/abs/2306.04700v1)|null|Dietary patterns synthesize multiple related diet components, which can be used by nutrition researchers to examine diet-disease relationships. Latent class models (LCMs) have been used to derive dietary patterns from dietary intake assessment, where each class profile represents the probabilities of exposure to a set of diet components. However, LCM-derived dietary patterns can exhibit strong similarities, or weak separation, resulting in numerical and inferential instabilities that challenge scientific interpretation. This issue is exacerbated in small-sized subpopulations. To address these issues, we provide a simple solution that empowers LCMs to improve dietary pattern estimation. We develop a tree-regularized Bayesian LCM that shares statistical strength between dietary patterns to make better estimates using limited data. This is achieved via a Dirichlet diffusion tree process that specifies a prior distribution for the unknown tree over classes. Dietary patterns that share proximity to one another in the tree are shrunk towards ancestral dietary patterns a priori, with the degree of shrinkage varying across pre-specified food groups. Using dietary intake data from the Hispanic Community Health Study/Study of Latinos, we apply the proposed approach to a sample of 496 US adults of South American ethnic background to identify and compare dietary patterns.|
|**2023-06-07**|**Causality Bounds in Quadratic Inflation from Purely Virtual Particles**|Alessandro Dondarini et.al.|[2306.04687v1](http://arxiv.org/abs/2306.04687v1)|null|The "$\phi^2$" slow roll inflation combined with General Relativity is largely excluded by Planck data. In this paper, we consider the same potential combined with the $R+C^2$ gravity of purely virtual particles (or fakeons), where the would-be ghost introduced by the Weyl tensor term, $C^2$, is quantized with the fakeon prescription. We compute the tensor power spectrum in the full theory by means of the Cosmic Renormalization Group formalism and critically examine its physical meaning. In particular, we show that it is not possible to retrieve the power spectrum of the fakeon free-theory by considering the decoupling limit of the purely virtual particles. We provide a physical explanation in terms of the causal structure of the theory to infer that a model of quadratic inflation from purely virtual particles is also discarded from a phenomenological point of view.|
|**2023-06-07**|**Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations**|Lifan Yuan et.al.|[2306.04618v1](http://arxiv.org/abs/2306.04618v1)|[link](https://github.com/lifan-yuan/ood_nlp)|This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}.|
|**2023-06-07**|**Proximity-Informed Calibration for Deep Neural Networks**|Miao Xiong et.al.|[2306.04590v1](http://arxiv.org/abs/2306.04590v1)|[link](https://github.com/miaoxiong2320/proximitybias-calibration)|Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures.|
|**2023-06-07**|**Evaluation of ChatGPT and Microsoft Bing AI Chat Performances on Physics Exams of Vietnamese National High School Graduation Examination**|Dao Xuan-Quy et.al.|[2306.04538v2](http://arxiv.org/abs/2306.04538v2)|null|The promise and difficulties of language model-based approaches for physics teaching were assessed in this study. This study evaluates how well ChatGPT and BingChat, two state-of-the-art (SOTA) large language models (LLMs), perform when answering high school physics questions on Vietnamese exams from 2019 to 2023. When we compared the results of the LLMs with the scores of Vietnamese students, we discovered that ChatGPT and BingChat both perform worse than Vietnamese students, proving that LLMs are not yet capable of fully replacing human intellect in the field of physics teaching. The outcomes also showed that neither LLM is capable of responding to questions at the high application levels. In terms of accuracy, BingChat typically surpassed ChatGPT, although ChatGPT showed more stability. Our research suggests that LLMs can help students and teachers during learning and teaching activities, particularly by offering immediate feedback and individualized learning experiences.|
|**2023-06-07**|**Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs**|Ines Reinig et.al.|[2306.04523v1](http://arxiv.org/abs/2306.04523v1)|[link](https://github.com/ireinig/wogli)|Compared to English, German word order is freer and therefore poses additional challenges for natural language inference (NLI). We create WOGLI (Word Order in German Language Inference), the first adversarial NLI dataset for German word order that has the following properties: (i) each premise has an entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ only in word order and necessary morphological changes to mark case and number. In particular, each premise andits two hypotheses contain exactly the same lemmata. Our adversarial examples require the model to use morphological markers in order to recognise or reject entailment. We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language. We also examine performance after data augmentation as well as on related word order phenomena derived from WOGLI. Our datasets are publically available at https://github.com/ireinig/wogli.|
|**2023-06-07**|**RD-Suite: A Benchmark for Ranking Distillation**|Zhen Qin et.al.|[2306.04455v1](http://arxiv.org/abs/2306.04455v1)|null|The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.|

## HINE1

### hnne
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**RDumb: A simple approach that questions our progress in continual test-time adaptation**|Ori Press et.al.|[2306.05401v1](http://arxiv.org/abs/2306.05401v1)|[link](https://github.com/oripress/ccc)|Test-Time Adaptation (TTA) allows to update pretrained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continuously Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, "RDumb", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing adaptation to avoid collapse nor able to outperform a simplistic resetting strategy.|
|**2023-06-08**|**Research Impact of Solar Panel Cleaning Robot on Photovoltaic Panel's Deflection**|Trung Dat Phan et.al.|[2306.05340v1](http://arxiv.org/abs/2306.05340v1)|null|In the last few decades, solar panel cleaning robots (SPCR) have been widely used for sanitizing photovoltaic (PV) panels as an effective solution for ensuring PV efficiency. However, the dynamic load generated by the SPCR during operation might have a negative impact on PV panels. To reduce these effects, this paper presents the utilization of ANSYS software to simulate multiple scenarios involving the impact of SPCR on PV panels. The simulation scenarios provided in the paper are derived from the typical movements of SPCR observed during practical operations. The simulation results show the deformation process of PV panels, and a second-order polynomial is established to describe the deformed amplitude along the centerline of PV panels. This second-order polynomial contributes to the design process of a damper system for SPCR aiming to reduce the influence of SPCR on PV panels. Moreover, the experiments are conducted to examine the correlation between the results of the simulation and the experiment.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our solver relies on a relaxation method in which convergence is sought as the steady-state solution of a parabolic equation, whose time-discretization is governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings indicate that the RKL-based Poisson solver, which is both fully parallel and rapidly convergent, has the potential to serve as a practical alternative to conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and \textit{successive over-relaxation} (SOR) methods. Additionally, it can mitigate some of the drawbacks of these traditional techniques. We incorporate our algorithm into a multigrid solver to provide a simple and efficient gravity solver that can be used to obtain the gravitational potentials in self-gravitational hydrodynamics. We test our implementation against a broad range of standard self-gravitating astrophysical problems designed to examine different aspects of the code. We demonstrate that the results match excellently with the analytical predictions (when available), and the findings of similar previous studies.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation. The robot decides whether to proceed by examining the tracker's bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker's ability to track target dishes and to determine the exit status. Our tracker shows 38% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Analysis of Knuth's Sampling Algorithm D and D'**|Mridul Nandi et.al.|[2306.05243v1](http://arxiv.org/abs/2306.05243v1)|null|In this research paper, we address the Distinct Elements estimation problem in the context of streaming algorithms. The problem involves estimating the number of distinct elements in a given data stream $\mathcal{A} = (a_1, a_2,\ldots, a_m)$, where $a_i \in \{1, 2, \ldots, n\}$. Over the past four decades, the Distinct Elements problem has received considerable attention, theoretically and empirically, leading to the development of space-optimal algorithms. A recent sampling-based algorithm proposed by Chakraborty et al.[11] has garnered significant interest and has even attracted the attention of renowned computer scientist Donald E. Knuth, who wrote an article on the same topic [6] and called the algorithm CVM. In this paper, we thoroughly examine the algorithms (referred to as CVM1, CVM2 in [6] and DonD, DonD' in [6]. We first unify all these algorithms and call them cutoff-based algorithms. Then we provide an approximation and biasedness analysis of these algorithms.|
|**2023-06-08**|**Design of Sturm global attractors 2: Time-reversible Chafee-Infante lattices of 3-nose meanders**|Bernold Fiedler et.al.|[2306.05232v1](http://arxiv.org/abs/2306.05232v1)|null|This sequel continues our exploration arxiv:2302.12531 of a deceptively ``simple'' class of global attractors, called Sturm due to nodal properties. They arise for the semilinear scalar parabolic PDE   \begin{equation}\label{eq:*}   u_t = u_{xx} + f(x,u,u_x) \tag{$*$}   \end{equation} on the unit interval $0 < x<1$, under Neumann boundary conditions. This models the interplay of reaction, advection, and diffusion.   Our classification is based on the Sturm meanders, which arise from a shooting approach to the ODE boundary value problem of equilibrium solutions $u=v(x)$. Specifically, we address meanders with only three ``noses'', each of which is innermost to a nested family of upper or lower meander arcs. The Chafee-Infante paradigm of 1974, with cubic nonlinearity $f=f(u)$, features just two noses.   We present, and fully prove, a precise description of global PDE connection graphs, graded by Morse index, for such gradient-like Morse-Smale systems \eqref{eq:*}. The directed edges denote PDE heteroclinic orbits $v_1 \leadsto v_2$ between equilibrium vertices $v_1, v_2$ of adjacent Morse index. The connection graphs can be described as a lattice-like structure of Chafee-Infante subgraphs. However, this simple description requires us to adjoin a single ``equilibrium'' vertex, formally, at Morse level -1. Surprisingly, for parabolic PDEs based on irreversible diffusion, the connection graphs then also exhibit global time reversibility.|
|**2023-06-08**|**Prospects for charged Higgs bosons in natural SUSY models at the high-luminosity LHC**|Howard Baer et.al.|[2306.05207v1](http://arxiv.org/abs/2306.05207v1)|null|We continue our examination of prospects for discovery of heavy Higgs bosons of natural SUSY (natSUSY) models at the high luminosity LHC (HL-LHC), this time focussing on charged Higgs bosons. In natSUSY, higgsinos are expected at the few hundred GeV scale whilst electroweak gauginos inhabit the TeV scale and the heavy Higgs bosons, H, A and H^\pm could range up tens of TeV without jeopardizing naturalness. For TeV-scale heavy SUSY Higgs bosons H, A and H^\pm, as currently required by LHC searches, SUSY decays into gaugino plus higgsino can dominate H^\pm decays provided these decays are kinematically accessible. The visible decay products of higgsinos are soft making them largely invisible, whilst the gauginos decay to W, Z or h plus missing transverse energy (MET). Charged Higgs bosons are dominantly produced at LHC14 via the parton subprocess, gb-> H^\pm t. In this paper, we examine the viability of observing signtures from H^\pm -> \tau\nu, H^\pm -> tb and H^\pm -> W, Z, h + MET events produced in association with a top quark at the HL-LHC over large Standard Model (SM) backgrounds from (mainly) t\bar{t}, t\bar{t}V and t\bar{t}h production (where V=W, Z). We find that the greatest reach is found via the SM H^\pm(-> \tau\nu) +t channel with a subdominant contribution from the H^\pm(-> tb) +t channel. Unlike for neutral Higgs searches, the SUSY decay modes appear to be unimportant for H^\pm searches at the HL-LHC. We delineate regions of the m_A vs. \tan\beta plane, mostly around m_A \sim 1-2 TeV, where signals from charged Higgs bosons would serve to confirm signals of a heavy, neutral Higgs boson at the 5\sigma level or, alternatively, to exclude heavy Higgs bosons at the 95% confidence level at the high luminosity LHC.|
|**2023-06-08**|**M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models**|Wenxuan Zhang et.al.|[2306.05179v1](http://arxiv.org/abs/2306.05179v1)|[link](https://github.com/damo-nlp-sg/m3exam)|Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.|
|**2023-06-08**|**Characterization of Multi-Channel Denial-of-Service and Full-Scale Denial-of-Service**|Anindya Basu et.al.|[2306.05161v1](http://arxiv.org/abs/2306.05161v1)|null|Over the past decades, interest in enhancing the safety of cyber-physical systems (CPSs) has risen. The systems and control research society has recognised that the embedded closed-loop in integrated systems may be damaged if attackers can execute a successful malicious attack. This article examines the resilient control problem for CPSs with numerous transmission channels under Denial-of-Service (DoS). First, a partial observer technique is developed in response to the Multi-Channel DoS (MCDoS) condition. The changing frequency of MCDoS is characterized while maintaining the Global Asymptotic Stability (GAS) of the closed loop system. The partial observer is modified then to reduce the effect of the changing frequency of MCDoS in the system. Then a resilient event-based feedback control scheme is developed to address the Full-Scale DoS (FSDoS). We depict the changing frequency of MCDoS and the frequency and duration of FSDoS, allowing the feedback system's Global Asymptotic Stability (GAS) to be maintained. We regard event-based controllers for which a minimal inter-sample time is precisely formulated in response to the existence of digital channels.|
|**2023-06-08**|**Electronic correlations and superconducting instability in La$_3$Ni$_2$O$_7$ under high pressure**|Frank Lechermann et.al.|[2306.05121v1](http://arxiv.org/abs/2306.05121v1)|null|Motivated by the report of superconductivity in bilayer La$_3$Ni$_2$O$_7$ at high pressure, we examine the interacting electrons in this system. First-principles many-body theory is utilized to study the normal-state electronic properties. Below 100\,K, a multi-orbital non-Fermi liquid state resulting from loss of Ni-ligand coherence within a flat-band dominated low-energy landscape is uncovered. The incoherent low-temperature Fermi surface displays strong mixing between Ni-$d_{z^2}$ and Ni-$d_{x^2-y^2}$ orbital character. In a model-Hamiltonian picture, spin fluctuations originating mostly from the Ni-$d_{z^2}$ orbital give rise to strong tendencies towards a superconducting instability with $d_{x^2-y^2}$ order parameter. The dramatic enhancement of $T_{\rm c}$ in pressurized La$_3$Ni$_2$O$_7$ is due to stronger Ni-$d_{z^2}$ correlations compared to those in the infinite-layer nickelates.|
|**2023-06-08**|**Stabilization of approximate GHZ state with quasi-local couplings**|Vincent Martin et.al.|[2306.05070v1](http://arxiv.org/abs/2306.05070v1)|null|We propose a reservoir design, composed of fixed dissipation operators acting each on few local subsystems, to stabilize an approximate GHZ state on n qubits. The main idea is to work out how a previously proposed sequence of two stabilization steps can be applied instead in appropriate (probabilistic) superposition. We examine alternatives to synchronize the superposition using local couplings only, thanks to a chain of "clock" ancillas or to additional levels on the data subsystems. The practical value of these alternatives depends on experimental constraints. They all feature a design tradeoff between approximate stabilization fidelity and protection against perturbations. These proposals illustrate how simple autonomous automata can be implemented in quantum reservoir engineering to replace sequential state preparation procedures. Encoding automaton actions via additional data levels only, appears particularly efficient in this context. Our analysis method, reducing the Lindblad master equation to a Markov chain on virtual output signals, may be of independent interest.|
|**2023-06-08**|**Leveraging Language Identification to Enhance Code-Mixed Text Classification**|Gauri Takawane et.al.|[2306.04964v1](http://arxiv.org/abs/2306.04964v1)|null|The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comparing their results with and without using word-level language information. The models were evaluated using metrics such as accuracy, precision, recall, and F1 score. Our findings show that the proposed language augmentation approaches work well across different BERT models. We demonstrate the importance of augmenting code-mixed text with language information on five different code-mixed Hindi-English downstream datasets based on sentiment analysis, hate speech detection, and emotion detection.|
|**2023-06-08**|**Towards a Success Model for Automated Programming Assessment Systems Used as a Formative Assessment Tool**|Clemens Sauerwein et.al.|[2306.04958v1](http://arxiv.org/abs/2306.04958v1)|null|The assessment of source code in university education is a central and important task for lecturers of programming courses. In doing so, educators are confronted with growing numbers of students having increasingly diverse prerequisites, a shortage of tutors, and highly dynamic learning objectives. To support lecturers in meeting these challenges, the use of automated programming assessment systems (APASs), facilitating formative assessments by providing timely, objective feedback, is a promising solution. Measuring the effectiveness and success of these platforms is crucial to understanding how such platforms should be designed, implemented, and used. However, research and practice lack a common understanding of aspects influencing the success of APASs. To address these issues, we have devised a success model for APASs based on established models from information systems as well as blended learning research and conducted an online survey with 414 students using the same APAS. In addition, we examined the role of mediators intervening between technology-, system- or self-related factors, respectively, and the users' satisfaction with APASs. Ultimately, our research has yielded a model of success comprising seven constructs influencing user satisfaction with an APAS.|
|**2023-06-08**|**Robust Learning with Progressive Data Expansion Against Spurious Correlation**|Yihe Deng et.al.|[2306.04949v1](http://arxiv.org/abs/2306.04949v1)|null|While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as ResNets and Transformers. On average, our method achieves a 2.8% improvement in worst-group accuracy compared with the state-of-the-art method, while enjoying up to 10x faster training efficiency.|
|**2023-06-08**|**Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes**|Peizhong Ju et.al.|[2306.04901v1](http://arxiv.org/abs/2306.04901v1)|null|Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the number of features/parameters in both underparameterized and overparameterized regimes. Furthermore, we provide practical guidelines for determining the number of features in the common and task-specific parts for improved generalization performance. For example, when the total number of features in the source task's learning model is fixed, we show that it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part. Moreover, in specific scenarios, particularly those characterized by high noise levels and small true parameters, sacrificing certain true features in the common part in favor of employing more redundant features in the task-specific part can yield notable benefits.|
|**2023-06-08**|**Dear Magellanic Clouds, welcome back!**|Eugene Vasiliev et.al.|[2306.04837v1](http://arxiv.org/abs/2306.04837v1)|null|We propose a scenario in which the Large Magellanic Cloud (LMC) is on its second passage around the Milky Way. Using a series of tailored N-body simulations, we demonstrate that such orbits are consistent with current observational constraints on the mass distribution and relative velocity of both galaxies. The previous pericentre passage of the LMC could have occurred 5-10 Gyr ago at a distance >~100 kpc, large enough to retain its current population of satellites. The perturbations of the Milky Way halo induced by the LMC look nearly identical to the first-passage scenario, however, the distribution of LMC debris is considerably broader in the second-passage model. We examine the likelihood of current and past association with the Magellanic system for dwarf galaxies in the Local Group, and find that in addition to 10-11 current LMC satellites, it could have brought a further 4-6 galaxies that have been lost after the first pericentre passage. In particular, four of the classical dwarfs - Carina, Draco, Fornax and Ursa Minor - each have a ~50% probability of once belonging to the Magellanic system, thus providing a possible explanation for the ``plane of satellites'' conundrum.|
|**2023-06-07**|**Perspectives in closed-loop supply chains network design considering risk and uncertainty factors**|Yang Hu et.al.|[2306.04819v1](http://arxiv.org/abs/2306.04819v1)|null|Risk and uncertainty in each stage of CLSC have greatly increased the complexity and reduced process efficiency of the closed-loop networks, impeding the sustainable and resilient development of industries and the circular economy. Recently, increasing interest in academia have been raised on the risk and uncertainty analysis of closed-loop supply chain, yet there is no comprehensive review paper focusing on closed-loop network design considering risk and uncertainty. This paper examines previous research on the domain of closed-loop network design under risk and uncertainties to provide constructive prospects for future study. We selected 106 papers published in the Scopus database from the year 2004 to 2022. We analyse the source of risk and uncertainties of the CLSC network and identified appropriate methods for handling uncertainties in addition to algorithms for solving uncertain CLSCND problems. We also illustrate the evolution of objectives for designing a closed-loop supply chain that is expos to risk or uncertainty, and investigate the application of uncertain network design models in practical industry sectors. Finally, we draw proper research gaps for each category and clarify some novel insights for future study. By considering the impacts of risk or uncertainties of different sources on closed-loop supply chain network design, we can approach the economical, sustainable, social, and resilient objectives effectively and efficiently.|
|**2023-06-07**|**Compressibility and speeds of sound across the superfluid to supersolid phase transition of an elongated dipolar gas**|P. B. Blakie et.al.|[2306.04794v1](http://arxiv.org/abs/2306.04794v1)|null|We investigate the excitation spectrum and compressibility of a dipolar Bose-Einstein condensate in an infinite tube potential in the parameter regime where the transition between superfluid and supersolid phases occurs. Our study focuses on the density range in which crystalline order develops continuously across the transition. Above the transition the superfluid shows a single gapless excitation band, phononic at small momenta and with a roton at a finite momentum. Below the transition, two gapless excitations branches (three at the transition point) emerge in the supersolid. We examine the two gapless excitation bands and their associated speeds of sound in the supersolid phase. Our results show that the speeds of sound and the compressibility are discontinuous at the transition, indicating a second-order phase transition. These results provide valuable insights into the identification of supersolid phenomena in dipolar quantum gases and the relationship to supersolidity in spin-orbit coupled gases.|
|**2023-06-07**|**The Temperature, Electron, and Pressure Characteristics of Switchbacks: Parker Solar Probe Observations**|Jia Huang et.al.|[2306.04773v1](http://arxiv.org/abs/2306.04773v1)|null|Parker Solar Probe (PSP) observes unexpectedly prevalent switchbacks, which are rapid magnetic field reversals that last from seconds to hours, in the inner heliosphere, posing new challenges to understanding their nature, origin, and evolution. In this work, we investigate the thermal states, electron pitch angle distributions, and pressure signatures of both inside and outside switchbacks, separating a switchback into spike, transition region (TR), and quiet period (QP). Based on our analysis, we find that the proton temperature anisotropies in TRs seem to show an intermediate state between spike and QP plasmas. The proton temperatures are more enhanced in spike than in TR and QP, but the alpha temperatures and alpha-to-proton temperature ratios show the opposite trends, implying that the preferential heating mechanisms of protons and alphas are competing in different regions of switchbacks. Moreover, our results suggest that the electron integrated intensities are almost the same across the switchbacks but the electron pitch angle distributions are more isotropic inside than outside switchbacks, implying switchbacks are intact structures but strong scattering of electrons happens inside switchbacks. In addition, the examination of pressures reveals that the total pressures are comparable through a switchback, confirming switchbacks are pressure-balanced structures. These characteristics could further our understanding of ion heating, electron scattering, and the structure of switchbacks.|
|**2023-06-07**|**Automatic retrieval of corresponding US views in longitudinal examinations**|Hamideh Kerdegari et.al.|[2306.04739v1](http://arxiv.org/abs/2306.04739v1)|null|Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%+/-0.24% error in cross-sectional area. Furthermore, a user study survey confirmed the efficacy of our model for muscle view retrieval.|
|**2023-06-07**|**Effects of Pressure on the Electronic and Magnetic Properties of Bulk NiI$_{2}$**|Jesse Kapeghian et.al.|[2306.04729v1](http://arxiv.org/abs/2306.04729v1)|null|Transition metal dihalides have recently garnered interest in the context of two-dimensional van der Waals magnets as their underlying geometrically frustrated triangular lattice leads to interesting competing exchange interactions. In particular, NiI$_{2}$ is a magnetic semiconductor that has been long known for its exotic helimagnetism in the bulk. Recent experiments have shown that the helimagnetic state survives down to the monolayer limit with a layer-dependent magnetic transition temperature that suggests a relevant role of the interlayer coupling. Here, we explore the effects of hydrostatic pressure as a means to enhance this interlayer exchange and ultimately tune the electronic and magnetic response of NiI$_{2}$. We study first the evolution of the structural parameters as a function of external pressure using first-principles calculations combined with x-ray diffraction measurements. We then examine the evolution of the electronic structure and magnetic exchange interactions via first-principles calculations and Monte Carlo simulations. We find that the leading interlayer coupling is an antiferromagnetic second-nearest neighbor interaction that increases monotonically with pressure. The ratio between isotropic third- and first-nearest neighbor intralayer exchanges, which controls the magnetic frustration and determines the magnetic propagation vector $\mathbf{q}$ of the helimagnetic ground state, is also enhanced by pressure. As a consequence, our Monte Carlo simulations show a monotonic increase in the magnetic transition temperature, indicating that pressure is an effective means to tune the magnetic response of NiI$_{2}$.|
|**2023-06-07**|**A New Family of Regression Models for $[0,1]$ Outcome Data: Expanding the Palette**|Eugene D. Hahn et.al.|[2306.04708v1](http://arxiv.org/abs/2306.04708v1)|null|Beta regression is a popular methodology when the outcome variable $y$ is on the open interval $(0,1)$. When $y$ is in the closed interval $[0,1]$, it is commonly accepted that beta regression is inapplicable. Instead, common solutions are to use augmented beta regression or censoring models or else to subjectively rescale the endpoints to allow beta regression. We provide an attractive new approach with a family of models that treats the entirety of $y\in[0,1]$ in a single model without rescaling or the need for the complications of augmentation or censoring. This family provides the interpretational convenience of a single straightforward model for the expectation of $y \in [0,1]$ over its entirety. We establish the conditions for the existence of a unique MLE and then examine this new family of models from both maximum-likelihood and Bayesian perspectives. We successfully apply the models to employment data in which augmented beta regression was difficult due to data separation. We also apply the models to healthcare panel data that were originally examined by way of rescaling.|
|**2023-06-07**|**Shadow and deflection angle of asymptotic, magnetically-charged, non-singular black hole**|Yashmitha Kumaran et.al.|[2306.04705v1](http://arxiv.org/abs/2306.04705v1)|null|In this paper, we present a detailed analysis of an asymptotic, magnetically-charged, non-singular (AMCNS) black hole. By utilizing the Gauss-bonnet theorem, we aim to unravel the intricate astrophysics associated with this unique black hole. The study explored various aspects including the black hole's gravitational field, intrinsic properties, light bending, the shadow and greybody bounding of the black hole. Through rigorous calculations and simulations, we derive the weak deflection angle of the optical metric of AMCNS black hole. Additionally, we investigate the impact of the dark matter medium on the deflection angle, examined the distinctive features of the black hole's shadow, and bound its greybody factors. Our findings not only deepen our understanding of gravitational lensing but also pave the way for future improvements in black hole theories by minimizing restrictive assumptions and incorporating a more realistic representation of these cosmic phenomena.|
|**2023-06-07**|**Tree-Regularized Bayesian Latent Class Analysis for Improving Weakly Separated Dietary Pattern Subtyping in Small-Sized Subpopulations**|Mengbing Li et.al.|[2306.04700v1](http://arxiv.org/abs/2306.04700v1)|null|Dietary patterns synthesize multiple related diet components, which can be used by nutrition researchers to examine diet-disease relationships. Latent class models (LCMs) have been used to derive dietary patterns from dietary intake assessment, where each class profile represents the probabilities of exposure to a set of diet components. However, LCM-derived dietary patterns can exhibit strong similarities, or weak separation, resulting in numerical and inferential instabilities that challenge scientific interpretation. This issue is exacerbated in small-sized subpopulations. To address these issues, we provide a simple solution that empowers LCMs to improve dietary pattern estimation. We develop a tree-regularized Bayesian LCM that shares statistical strength between dietary patterns to make better estimates using limited data. This is achieved via a Dirichlet diffusion tree process that specifies a prior distribution for the unknown tree over classes. Dietary patterns that share proximity to one another in the tree are shrunk towards ancestral dietary patterns a priori, with the degree of shrinkage varying across pre-specified food groups. Using dietary intake data from the Hispanic Community Health Study/Study of Latinos, we apply the proposed approach to a sample of 496 US adults of South American ethnic background to identify and compare dietary patterns.|
|**2023-06-07**|**Causality Bounds in Quadratic Inflation from Purely Virtual Particles**|Alessandro Dondarini et.al.|[2306.04687v1](http://arxiv.org/abs/2306.04687v1)|null|The "$\phi^2$" slow roll inflation combined with General Relativity is largely excluded by Planck data. In this paper, we consider the same potential combined with the $R+C^2$ gravity of purely virtual particles (or fakeons), where the would-be ghost introduced by the Weyl tensor term, $C^2$, is quantized with the fakeon prescription. We compute the tensor power spectrum in the full theory by means of the Cosmic Renormalization Group formalism and critically examine its physical meaning. In particular, we show that it is not possible to retrieve the power spectrum of the fakeon free-theory by considering the decoupling limit of the purely virtual particles. We provide a physical explanation in terms of the causal structure of the theory to infer that a model of quadratic inflation from purely virtual particles is also discarded from a phenomenological point of view.|
|**2023-06-07**|**Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations**|Lifan Yuan et.al.|[2306.04618v1](http://arxiv.org/abs/2306.04618v1)|[link](https://github.com/lifan-yuan/ood_nlp)|This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}.|
|**2023-06-07**|**Proximity-Informed Calibration for Deep Neural Networks**|Miao Xiong et.al.|[2306.04590v1](http://arxiv.org/abs/2306.04590v1)|[link](https://github.com/miaoxiong2320/proximitybias-calibration)|Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures.|
|**2023-06-07**|**Evaluation of ChatGPT and Microsoft Bing AI Chat Performances on Physics Exams of Vietnamese National High School Graduation Examination**|Dao Xuan-Quy et.al.|[2306.04538v2](http://arxiv.org/abs/2306.04538v2)|null|The promise and difficulties of language model-based approaches for physics teaching were assessed in this study. This study evaluates how well ChatGPT and BingChat, two state-of-the-art (SOTA) large language models (LLMs), perform when answering high school physics questions on Vietnamese exams from 2019 to 2023. When we compared the results of the LLMs with the scores of Vietnamese students, we discovered that ChatGPT and BingChat both perform worse than Vietnamese students, proving that LLMs are not yet capable of fully replacing human intellect in the field of physics teaching. The outcomes also showed that neither LLM is capable of responding to questions at the high application levels. In terms of accuracy, BingChat typically surpassed ChatGPT, although ChatGPT showed more stability. Our research suggests that LLMs can help students and teachers during learning and teaching activities, particularly by offering immediate feedback and individualized learning experiences.|
|**2023-06-07**|**Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs**|Ines Reinig et.al.|[2306.04523v1](http://arxiv.org/abs/2306.04523v1)|[link](https://github.com/ireinig/wogli)|Compared to English, German word order is freer and therefore poses additional challenges for natural language inference (NLI). We create WOGLI (Word Order in German Language Inference), the first adversarial NLI dataset for German word order that has the following properties: (i) each premise has an entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ only in word order and necessary morphological changes to mark case and number. In particular, each premise andits two hypotheses contain exactly the same lemmata. Our adversarial examples require the model to use morphological markers in order to recognise or reject entailment. We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language. We also examine performance after data augmentation as well as on related word order phenomena derived from WOGLI. Our datasets are publically available at https://github.com/ireinig/wogli.|
|**2023-06-07**|**RD-Suite: A Benchmark for Ranking Distillation**|Zhen Qin et.al.|[2306.04455v1](http://arxiv.org/abs/2306.04455v1)|null|The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.|

### hine
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-08**|**RDumb: A simple approach that questions our progress in continual test-time adaptation**|Ori Press et.al.|[2306.05401v1](http://arxiv.org/abs/2306.05401v1)|[link](https://github.com/oripress/ccc)|Test-Time Adaptation (TTA) allows to update pretrained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continuously Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, "RDumb", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing adaptation to avoid collapse nor able to outperform a simplistic resetting strategy.|
|**2023-06-08**|**Research Impact of Solar Panel Cleaning Robot on Photovoltaic Panel's Deflection**|Trung Dat Phan et.al.|[2306.05340v1](http://arxiv.org/abs/2306.05340v1)|null|In the last few decades, solar panel cleaning robots (SPCR) have been widely used for sanitizing photovoltaic (PV) panels as an effective solution for ensuring PV efficiency. However, the dynamic load generated by the SPCR during operation might have a negative impact on PV panels. To reduce these effects, this paper presents the utilization of ANSYS software to simulate multiple scenarios involving the impact of SPCR on PV panels. The simulation scenarios provided in the paper are derived from the typical movements of SPCR observed during practical operations. The simulation results show the deformation process of PV panels, and a second-order polynomial is established to describe the deformed amplitude along the centerline of PV panels. This second-order polynomial contributes to the design process of a damper system for SPCR aiming to reduce the influence of SPCR on PV panels. Moreover, the experiments are conducted to examine the correlation between the results of the simulation and the experiment.|
|**2023-06-08**|**A self-gravity module for the PLUTO code**|Ankush Mandal et.al.|[2306.05332v1](http://arxiv.org/abs/2306.05332v1)|null|We present a novel implementation of an iterative solver for the solution of the Poisson equation in the PLUTO code for astrophysical fluid dynamics. Our solver relies on a relaxation method in which convergence is sought as the steady-state solution of a parabolic equation, whose time-discretization is governed by the \textit{Runge-Kutta-Legendre} (RKL) method. Our findings indicate that the RKL-based Poisson solver, which is both fully parallel and rapidly convergent, has the potential to serve as a practical alternative to conventional iterative solvers such as the \textit{Gauss-Seidel} (GS) and \textit{successive over-relaxation} (SOR) methods. Additionally, it can mitigate some of the drawbacks of these traditional techniques. We incorporate our algorithm into a multigrid solver to provide a simple and efficient gravity solver that can be used to obtain the gravitational potentials in self-gravitational hydrodynamics. We test our implementation against a broad range of standard self-gravitating astrophysical problems designed to examine different aspects of the code. We demonstrate that the results match excellently with the analytical predictions (when available), and the findings of similar previous studies.|
|**2023-06-08**|**EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object**|Hyunseo Kim et.al.|[2306.05262v1](http://arxiv.org/abs/2306.05262v1)|null|Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation. The robot decides whether to proceed by examining the tracker's bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker's ability to track target dishes and to determine the exit status. Our tracker shows 38% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.|
|**2023-06-08**|**Analysis of Knuth's Sampling Algorithm D and D'**|Mridul Nandi et.al.|[2306.05243v1](http://arxiv.org/abs/2306.05243v1)|null|In this research paper, we address the Distinct Elements estimation problem in the context of streaming algorithms. The problem involves estimating the number of distinct elements in a given data stream $\mathcal{A} = (a_1, a_2,\ldots, a_m)$, where $a_i \in \{1, 2, \ldots, n\}$. Over the past four decades, the Distinct Elements problem has received considerable attention, theoretically and empirically, leading to the development of space-optimal algorithms. A recent sampling-based algorithm proposed by Chakraborty et al.[11] has garnered significant interest and has even attracted the attention of renowned computer scientist Donald E. Knuth, who wrote an article on the same topic [6] and called the algorithm CVM. In this paper, we thoroughly examine the algorithms (referred to as CVM1, CVM2 in [6] and DonD, DonD' in [6]. We first unify all these algorithms and call them cutoff-based algorithms. Then we provide an approximation and biasedness analysis of these algorithms.|
|**2023-06-08**|**Design of Sturm global attractors 2: Time-reversible Chafee-Infante lattices of 3-nose meanders**|Bernold Fiedler et.al.|[2306.05232v1](http://arxiv.org/abs/2306.05232v1)|null|This sequel continues our exploration arxiv:2302.12531 of a deceptively ``simple'' class of global attractors, called Sturm due to nodal properties. They arise for the semilinear scalar parabolic PDE   \begin{equation}\label{eq:*}   u_t = u_{xx} + f(x,u,u_x) \tag{$*$}   \end{equation} on the unit interval $0 < x<1$, under Neumann boundary conditions. This models the interplay of reaction, advection, and diffusion.   Our classification is based on the Sturm meanders, which arise from a shooting approach to the ODE boundary value problem of equilibrium solutions $u=v(x)$. Specifically, we address meanders with only three ``noses'', each of which is innermost to a nested family of upper or lower meander arcs. The Chafee-Infante paradigm of 1974, with cubic nonlinearity $f=f(u)$, features just two noses.   We present, and fully prove, a precise description of global PDE connection graphs, graded by Morse index, for such gradient-like Morse-Smale systems \eqref{eq:*}. The directed edges denote PDE heteroclinic orbits $v_1 \leadsto v_2$ between equilibrium vertices $v_1, v_2$ of adjacent Morse index. The connection graphs can be described as a lattice-like structure of Chafee-Infante subgraphs. However, this simple description requires us to adjoin a single ``equilibrium'' vertex, formally, at Morse level -1. Surprisingly, for parabolic PDEs based on irreversible diffusion, the connection graphs then also exhibit global time reversibility.|
|**2023-06-08**|**Prospects for charged Higgs bosons in natural SUSY models at the high-luminosity LHC**|Howard Baer et.al.|[2306.05207v1](http://arxiv.org/abs/2306.05207v1)|null|We continue our examination of prospects for discovery of heavy Higgs bosons of natural SUSY (natSUSY) models at the high luminosity LHC (HL-LHC), this time focussing on charged Higgs bosons. In natSUSY, higgsinos are expected at the few hundred GeV scale whilst electroweak gauginos inhabit the TeV scale and the heavy Higgs bosons, H, A and H^\pm could range up tens of TeV without jeopardizing naturalness. For TeV-scale heavy SUSY Higgs bosons H, A and H^\pm, as currently required by LHC searches, SUSY decays into gaugino plus higgsino can dominate H^\pm decays provided these decays are kinematically accessible. The visible decay products of higgsinos are soft making them largely invisible, whilst the gauginos decay to W, Z or h plus missing transverse energy (MET). Charged Higgs bosons are dominantly produced at LHC14 via the parton subprocess, gb-> H^\pm t. In this paper, we examine the viability of observing signtures from H^\pm -> \tau\nu, H^\pm -> tb and H^\pm -> W, Z, h + MET events produced in association with a top quark at the HL-LHC over large Standard Model (SM) backgrounds from (mainly) t\bar{t}, t\bar{t}V and t\bar{t}h production (where V=W, Z). We find that the greatest reach is found via the SM H^\pm(-> \tau\nu) +t channel with a subdominant contribution from the H^\pm(-> tb) +t channel. Unlike for neutral Higgs searches, the SUSY decay modes appear to be unimportant for H^\pm searches at the HL-LHC. We delineate regions of the m_A vs. \tan\beta plane, mostly around m_A \sim 1-2 TeV, where signals from charged Higgs bosons would serve to confirm signals of a heavy, neutral Higgs boson at the 5\sigma level or, alternatively, to exclude heavy Higgs bosons at the 95% confidence level at the high luminosity LHC.|
|**2023-06-08**|**M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models**|Wenxuan Zhang et.al.|[2306.05179v1](http://arxiv.org/abs/2306.05179v1)|[link](https://github.com/damo-nlp-sg/m3exam)|Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.|
|**2023-06-08**|**Characterization of Multi-Channel Denial-of-Service and Full-Scale Denial-of-Service**|Anindya Basu et.al.|[2306.05161v1](http://arxiv.org/abs/2306.05161v1)|null|Over the past decades, interest in enhancing the safety of cyber-physical systems (CPSs) has risen. The systems and control research society has recognised that the embedded closed-loop in integrated systems may be damaged if attackers can execute a successful malicious attack. This article examines the resilient control problem for CPSs with numerous transmission channels under Denial-of-Service (DoS). First, a partial observer technique is developed in response to the Multi-Channel DoS (MCDoS) condition. The changing frequency of MCDoS is characterized while maintaining the Global Asymptotic Stability (GAS) of the closed loop system. The partial observer is modified then to reduce the effect of the changing frequency of MCDoS in the system. Then a resilient event-based feedback control scheme is developed to address the Full-Scale DoS (FSDoS). We depict the changing frequency of MCDoS and the frequency and duration of FSDoS, allowing the feedback system's Global Asymptotic Stability (GAS) to be maintained. We regard event-based controllers for which a minimal inter-sample time is precisely formulated in response to the existence of digital channels.|
|**2023-06-08**|**Electronic correlations and superconducting instability in La$_3$Ni$_2$O$_7$ under high pressure**|Frank Lechermann et.al.|[2306.05121v1](http://arxiv.org/abs/2306.05121v1)|null|Motivated by the report of superconductivity in bilayer La$_3$Ni$_2$O$_7$ at high pressure, we examine the interacting electrons in this system. First-principles many-body theory is utilized to study the normal-state electronic properties. Below 100\,K, a multi-orbital non-Fermi liquid state resulting from loss of Ni-ligand coherence within a flat-band dominated low-energy landscape is uncovered. The incoherent low-temperature Fermi surface displays strong mixing between Ni-$d_{z^2}$ and Ni-$d_{x^2-y^2}$ orbital character. In a model-Hamiltonian picture, spin fluctuations originating mostly from the Ni-$d_{z^2}$ orbital give rise to strong tendencies towards a superconducting instability with $d_{x^2-y^2}$ order parameter. The dramatic enhancement of $T_{\rm c}$ in pressurized La$_3$Ni$_2$O$_7$ is due to stronger Ni-$d_{z^2}$ correlations compared to those in the infinite-layer nickelates.|
|**2023-06-08**|**Stabilization of approximate GHZ state with quasi-local couplings**|Vincent Martin et.al.|[2306.05070v1](http://arxiv.org/abs/2306.05070v1)|null|We propose a reservoir design, composed of fixed dissipation operators acting each on few local subsystems, to stabilize an approximate GHZ state on n qubits. The main idea is to work out how a previously proposed sequence of two stabilization steps can be applied instead in appropriate (probabilistic) superposition. We examine alternatives to synchronize the superposition using local couplings only, thanks to a chain of "clock" ancillas or to additional levels on the data subsystems. The practical value of these alternatives depends on experimental constraints. They all feature a design tradeoff between approximate stabilization fidelity and protection against perturbations. These proposals illustrate how simple autonomous automata can be implemented in quantum reservoir engineering to replace sequential state preparation procedures. Encoding automaton actions via additional data levels only, appears particularly efficient in this context. Our analysis method, reducing the Lindblad master equation to a Markov chain on virtual output signals, may be of independent interest.|
|**2023-06-08**|**Leveraging Language Identification to Enhance Code-Mixed Text Classification**|Gauri Takawane et.al.|[2306.04964v1](http://arxiv.org/abs/2306.04964v1)|null|The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comparing their results with and without using word-level language information. The models were evaluated using metrics such as accuracy, precision, recall, and F1 score. Our findings show that the proposed language augmentation approaches work well across different BERT models. We demonstrate the importance of augmenting code-mixed text with language information on five different code-mixed Hindi-English downstream datasets based on sentiment analysis, hate speech detection, and emotion detection.|
|**2023-06-08**|**Towards a Success Model for Automated Programming Assessment Systems Used as a Formative Assessment Tool**|Clemens Sauerwein et.al.|[2306.04958v1](http://arxiv.org/abs/2306.04958v1)|null|The assessment of source code in university education is a central and important task for lecturers of programming courses. In doing so, educators are confronted with growing numbers of students having increasingly diverse prerequisites, a shortage of tutors, and highly dynamic learning objectives. To support lecturers in meeting these challenges, the use of automated programming assessment systems (APASs), facilitating formative assessments by providing timely, objective feedback, is a promising solution. Measuring the effectiveness and success of these platforms is crucial to understanding how such platforms should be designed, implemented, and used. However, research and practice lack a common understanding of aspects influencing the success of APASs. To address these issues, we have devised a success model for APASs based on established models from information systems as well as blended learning research and conducted an online survey with 414 students using the same APAS. In addition, we examined the role of mediators intervening between technology-, system- or self-related factors, respectively, and the users' satisfaction with APASs. Ultimately, our research has yielded a model of success comprising seven constructs influencing user satisfaction with an APAS.|
|**2023-06-08**|**Robust Learning with Progressive Data Expansion Against Spurious Correlation**|Yihe Deng et.al.|[2306.04949v1](http://arxiv.org/abs/2306.04949v1)|null|While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as ResNets and Transformers. On average, our method achieves a 2.8% improvement in worst-group accuracy compared with the state-of-the-art method, while enjoying up to 10x faster training efficiency.|
|**2023-06-08**|**Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes**|Peizhong Ju et.al.|[2306.04901v1](http://arxiv.org/abs/2306.04901v1)|null|Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the number of features/parameters in both underparameterized and overparameterized regimes. Furthermore, we provide practical guidelines for determining the number of features in the common and task-specific parts for improved generalization performance. For example, when the total number of features in the source task's learning model is fixed, we show that it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part. Moreover, in specific scenarios, particularly those characterized by high noise levels and small true parameters, sacrificing certain true features in the common part in favor of employing more redundant features in the task-specific part can yield notable benefits.|
|**2023-06-08**|**Dear Magellanic Clouds, welcome back!**|Eugene Vasiliev et.al.|[2306.04837v1](http://arxiv.org/abs/2306.04837v1)|null|We propose a scenario in which the Large Magellanic Cloud (LMC) is on its second passage around the Milky Way. Using a series of tailored N-body simulations, we demonstrate that such orbits are consistent with current observational constraints on the mass distribution and relative velocity of both galaxies. The previous pericentre passage of the LMC could have occurred 5-10 Gyr ago at a distance >~100 kpc, large enough to retain its current population of satellites. The perturbations of the Milky Way halo induced by the LMC look nearly identical to the first-passage scenario, however, the distribution of LMC debris is considerably broader in the second-passage model. We examine the likelihood of current and past association with the Magellanic system for dwarf galaxies in the Local Group, and find that in addition to 10-11 current LMC satellites, it could have brought a further 4-6 galaxies that have been lost after the first pericentre passage. In particular, four of the classical dwarfs - Carina, Draco, Fornax and Ursa Minor - each have a ~50% probability of once belonging to the Magellanic system, thus providing a possible explanation for the ``plane of satellites'' conundrum.|
|**2023-06-07**|**Perspectives in closed-loop supply chains network design considering risk and uncertainty factors**|Yang Hu et.al.|[2306.04819v1](http://arxiv.org/abs/2306.04819v1)|null|Risk and uncertainty in each stage of CLSC have greatly increased the complexity and reduced process efficiency of the closed-loop networks, impeding the sustainable and resilient development of industries and the circular economy. Recently, increasing interest in academia have been raised on the risk and uncertainty analysis of closed-loop supply chain, yet there is no comprehensive review paper focusing on closed-loop network design considering risk and uncertainty. This paper examines previous research on the domain of closed-loop network design under risk and uncertainties to provide constructive prospects for future study. We selected 106 papers published in the Scopus database from the year 2004 to 2022. We analyse the source of risk and uncertainties of the CLSC network and identified appropriate methods for handling uncertainties in addition to algorithms for solving uncertain CLSCND problems. We also illustrate the evolution of objectives for designing a closed-loop supply chain that is expos to risk or uncertainty, and investigate the application of uncertain network design models in practical industry sectors. Finally, we draw proper research gaps for each category and clarify some novel insights for future study. By considering the impacts of risk or uncertainties of different sources on closed-loop supply chain network design, we can approach the economical, sustainable, social, and resilient objectives effectively and efficiently.|
|**2023-06-07**|**Compressibility and speeds of sound across the superfluid to supersolid phase transition of an elongated dipolar gas**|P. B. Blakie et.al.|[2306.04794v1](http://arxiv.org/abs/2306.04794v1)|null|We investigate the excitation spectrum and compressibility of a dipolar Bose-Einstein condensate in an infinite tube potential in the parameter regime where the transition between superfluid and supersolid phases occurs. Our study focuses on the density range in which crystalline order develops continuously across the transition. Above the transition the superfluid shows a single gapless excitation band, phononic at small momenta and with a roton at a finite momentum. Below the transition, two gapless excitations branches (three at the transition point) emerge in the supersolid. We examine the two gapless excitation bands and their associated speeds of sound in the supersolid phase. Our results show that the speeds of sound and the compressibility are discontinuous at the transition, indicating a second-order phase transition. These results provide valuable insights into the identification of supersolid phenomena in dipolar quantum gases and the relationship to supersolidity in spin-orbit coupled gases.|
|**2023-06-07**|**The Temperature, Electron, and Pressure Characteristics of Switchbacks: Parker Solar Probe Observations**|Jia Huang et.al.|[2306.04773v1](http://arxiv.org/abs/2306.04773v1)|null|Parker Solar Probe (PSP) observes unexpectedly prevalent switchbacks, which are rapid magnetic field reversals that last from seconds to hours, in the inner heliosphere, posing new challenges to understanding their nature, origin, and evolution. In this work, we investigate the thermal states, electron pitch angle distributions, and pressure signatures of both inside and outside switchbacks, separating a switchback into spike, transition region (TR), and quiet period (QP). Based on our analysis, we find that the proton temperature anisotropies in TRs seem to show an intermediate state between spike and QP plasmas. The proton temperatures are more enhanced in spike than in TR and QP, but the alpha temperatures and alpha-to-proton temperature ratios show the opposite trends, implying that the preferential heating mechanisms of protons and alphas are competing in different regions of switchbacks. Moreover, our results suggest that the electron integrated intensities are almost the same across the switchbacks but the electron pitch angle distributions are more isotropic inside than outside switchbacks, implying switchbacks are intact structures but strong scattering of electrons happens inside switchbacks. In addition, the examination of pressures reveals that the total pressures are comparable through a switchback, confirming switchbacks are pressure-balanced structures. These characteristics could further our understanding of ion heating, electron scattering, and the structure of switchbacks.|
|**2023-06-07**|**Automatic retrieval of corresponding US views in longitudinal examinations**|Hamideh Kerdegari et.al.|[2306.04739v1](http://arxiv.org/abs/2306.04739v1)|null|Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%+/-0.24% error in cross-sectional area. Furthermore, a user study survey confirmed the efficacy of our model for muscle view retrieval.|
|**2023-06-07**|**Effects of Pressure on the Electronic and Magnetic Properties of Bulk NiI$_{2}$**|Jesse Kapeghian et.al.|[2306.04729v1](http://arxiv.org/abs/2306.04729v1)|null|Transition metal dihalides have recently garnered interest in the context of two-dimensional van der Waals magnets as their underlying geometrically frustrated triangular lattice leads to interesting competing exchange interactions. In particular, NiI$_{2}$ is a magnetic semiconductor that has been long known for its exotic helimagnetism in the bulk. Recent experiments have shown that the helimagnetic state survives down to the monolayer limit with a layer-dependent magnetic transition temperature that suggests a relevant role of the interlayer coupling. Here, we explore the effects of hydrostatic pressure as a means to enhance this interlayer exchange and ultimately tune the electronic and magnetic response of NiI$_{2}$. We study first the evolution of the structural parameters as a function of external pressure using first-principles calculations combined with x-ray diffraction measurements. We then examine the evolution of the electronic structure and magnetic exchange interactions via first-principles calculations and Monte Carlo simulations. We find that the leading interlayer coupling is an antiferromagnetic second-nearest neighbor interaction that increases monotonically with pressure. The ratio between isotropic third- and first-nearest neighbor intralayer exchanges, which controls the magnetic frustration and determines the magnetic propagation vector $\mathbf{q}$ of the helimagnetic ground state, is also enhanced by pressure. As a consequence, our Monte Carlo simulations show a monotonic increase in the magnetic transition temperature, indicating that pressure is an effective means to tune the magnetic response of NiI$_{2}$.|
|**2023-06-07**|**A New Family of Regression Models for $[0,1]$ Outcome Data: Expanding the Palette**|Eugene D. Hahn et.al.|[2306.04708v1](http://arxiv.org/abs/2306.04708v1)|null|Beta regression is a popular methodology when the outcome variable $y$ is on the open interval $(0,1)$. When $y$ is in the closed interval $[0,1]$, it is commonly accepted that beta regression is inapplicable. Instead, common solutions are to use augmented beta regression or censoring models or else to subjectively rescale the endpoints to allow beta regression. We provide an attractive new approach with a family of models that treats the entirety of $y\in[0,1]$ in a single model without rescaling or the need for the complications of augmentation or censoring. This family provides the interpretational convenience of a single straightforward model for the expectation of $y \in [0,1]$ over its entirety. We establish the conditions for the existence of a unique MLE and then examine this new family of models from both maximum-likelihood and Bayesian perspectives. We successfully apply the models to employment data in which augmented beta regression was difficult due to data separation. We also apply the models to healthcare panel data that were originally examined by way of rescaling.|
|**2023-06-07**|**Shadow and deflection angle of asymptotic, magnetically-charged, non-singular black hole**|Yashmitha Kumaran et.al.|[2306.04705v1](http://arxiv.org/abs/2306.04705v1)|null|In this paper, we present a detailed analysis of an asymptotic, magnetically-charged, non-singular (AMCNS) black hole. By utilizing the Gauss-bonnet theorem, we aim to unravel the intricate astrophysics associated with this unique black hole. The study explored various aspects including the black hole's gravitational field, intrinsic properties, light bending, the shadow and greybody bounding of the black hole. Through rigorous calculations and simulations, we derive the weak deflection angle of the optical metric of AMCNS black hole. Additionally, we investigate the impact of the dark matter medium on the deflection angle, examined the distinctive features of the black hole's shadow, and bound its greybody factors. Our findings not only deepen our understanding of gravitational lensing but also pave the way for future improvements in black hole theories by minimizing restrictive assumptions and incorporating a more realistic representation of these cosmic phenomena.|
|**2023-06-07**|**Tree-Regularized Bayesian Latent Class Analysis for Improving Weakly Separated Dietary Pattern Subtyping in Small-Sized Subpopulations**|Mengbing Li et.al.|[2306.04700v1](http://arxiv.org/abs/2306.04700v1)|null|Dietary patterns synthesize multiple related diet components, which can be used by nutrition researchers to examine diet-disease relationships. Latent class models (LCMs) have been used to derive dietary patterns from dietary intake assessment, where each class profile represents the probabilities of exposure to a set of diet components. However, LCM-derived dietary patterns can exhibit strong similarities, or weak separation, resulting in numerical and inferential instabilities that challenge scientific interpretation. This issue is exacerbated in small-sized subpopulations. To address these issues, we provide a simple solution that empowers LCMs to improve dietary pattern estimation. We develop a tree-regularized Bayesian LCM that shares statistical strength between dietary patterns to make better estimates using limited data. This is achieved via a Dirichlet diffusion tree process that specifies a prior distribution for the unknown tree over classes. Dietary patterns that share proximity to one another in the tree are shrunk towards ancestral dietary patterns a priori, with the degree of shrinkage varying across pre-specified food groups. Using dietary intake data from the Hispanic Community Health Study/Study of Latinos, we apply the proposed approach to a sample of 496 US adults of South American ethnic background to identify and compare dietary patterns.|
|**2023-06-07**|**Causality Bounds in Quadratic Inflation from Purely Virtual Particles**|Alessandro Dondarini et.al.|[2306.04687v1](http://arxiv.org/abs/2306.04687v1)|null|The "$\phi^2$" slow roll inflation combined with General Relativity is largely excluded by Planck data. In this paper, we consider the same potential combined with the $R+C^2$ gravity of purely virtual particles (or fakeons), where the would-be ghost introduced by the Weyl tensor term, $C^2$, is quantized with the fakeon prescription. We compute the tensor power spectrum in the full theory by means of the Cosmic Renormalization Group formalism and critically examine its physical meaning. In particular, we show that it is not possible to retrieve the power spectrum of the fakeon free-theory by considering the decoupling limit of the purely virtual particles. We provide a physical explanation in terms of the causal structure of the theory to infer that a model of quadratic inflation from purely virtual particles is also discarded from a phenomenological point of view.|
|**2023-06-07**|**Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations**|Lifan Yuan et.al.|[2306.04618v1](http://arxiv.org/abs/2306.04618v1)|[link](https://github.com/lifan-yuan/ood_nlp)|This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}.|
|**2023-06-07**|**Proximity-Informed Calibration for Deep Neural Networks**|Miao Xiong et.al.|[2306.04590v1](http://arxiv.org/abs/2306.04590v1)|[link](https://github.com/miaoxiong2320/proximitybias-calibration)|Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures.|
|**2023-06-07**|**Evaluation of ChatGPT and Microsoft Bing AI Chat Performances on Physics Exams of Vietnamese National High School Graduation Examination**|Dao Xuan-Quy et.al.|[2306.04538v2](http://arxiv.org/abs/2306.04538v2)|null|The promise and difficulties of language model-based approaches for physics teaching were assessed in this study. This study evaluates how well ChatGPT and BingChat, two state-of-the-art (SOTA) large language models (LLMs), perform when answering high school physics questions on Vietnamese exams from 2019 to 2023. When we compared the results of the LLMs with the scores of Vietnamese students, we discovered that ChatGPT and BingChat both perform worse than Vietnamese students, proving that LLMs are not yet capable of fully replacing human intellect in the field of physics teaching. The outcomes also showed that neither LLM is capable of responding to questions at the high application levels. In terms of accuracy, BingChat typically surpassed ChatGPT, although ChatGPT showed more stability. Our research suggests that LLMs can help students and teachers during learning and teaching activities, particularly by offering immediate feedback and individualized learning experiences.|
|**2023-06-07**|**Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs**|Ines Reinig et.al.|[2306.04523v1](http://arxiv.org/abs/2306.04523v1)|[link](https://github.com/ireinig/wogli)|Compared to English, German word order is freer and therefore poses additional challenges for natural language inference (NLI). We create WOGLI (Word Order in German Language Inference), the first adversarial NLI dataset for German word order that has the following properties: (i) each premise has an entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ only in word order and necessary morphological changes to mark case and number. In particular, each premise andits two hypotheses contain exactly the same lemmata. Our adversarial examples require the model to use morphological markers in order to recognise or reject entailment. We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language. We also examine performance after data augmentation as well as on related word order phenomena derived from WOGLI. Our datasets are publically available at https://github.com/ireinig/wogli.|
|**2023-06-07**|**RD-Suite: A Benchmark for Ranking Distillation**|Zhen Qin et.al.|[2306.04455v1](http://arxiv.org/abs/2306.04455v1)|null|The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.|
