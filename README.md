# arxiv-daily latest papers around hine arxiv daily paper
Automated deployment @ 2023-06-25 20:31:50 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`]({repo_url}/blob/main/database/topic.yml).
> You can also view historical data through the [storage]({repo_url}/blob/main/database/storage).

## HINE

### hine
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-06-22**|**Armed Conflict and Early Human Capital Accumulation: Evidence from Cameroon's Anglophone Conflict**|Hector Galindo-Silva et.al.|[2306.13070v1](http://arxiv.org/abs/2306.13070v1)|null|This paper examines the impact of the Anglophone Conflict in Cameroon on human capital accumulation. Using high-quality individual-level data on test scores and information on conflict-related violent events, a difference-in-differences design is employed to estimate the conflict's causal effects. The results show that an increase in violent events and conflict-related deaths causes a significant decline in test scores in reading and mathematics. The conflict also leads to higher rates of teacher absenteeism and reduced access to electricity in schools. These findings highlight the adverse consequences of conflict-related violence on human capital accumulation, particularly within the Anglophone subsystem. The study emphasizes the disproportionate burden faced by Anglophone pupils due to language-rooted tensions and segregated educational systems.|
|**2023-06-22**|**Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems**|Theshani Nuradha et.al.|[2306.13054v1](http://arxiv.org/abs/2306.13054v1)|null|We propose a versatile privacy framework for quantum systems, termed quantum pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our formulation generalizes and addresses limitations of quantum differential privacy by offering flexibility in specifying private information, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze the privacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied to privacy auditing for identifying privacy violations via a hypothesis testing pipeline that leverages quantum algorithms. Connections to quantum fairness and other quantum divergences are also explored and several variants of QPP are examined.|
|**2023-06-22**|**CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions**|Adian Liusie et.al.|[2306.13047v1](http://arxiv.org/abs/2306.13047v1)|null|Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task. We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select. We release the data publicly for future research.|
|**2023-06-22**|**What to Learn: Features, Image Transformations, or Both?**|Yuxuan Chen et.al.|[2306.13040v1](http://arxiv.org/abs/2306.13040v1)|null|Long-term visual localization is an essential problem in robotics and computer vision, but remains challenging due to the environmental appearance changes caused by lighting and seasons. While many existing works have attempted to solve it by directly learning invariant sparse keypoints and descriptors to match scenes, these approaches still struggle with adverse appearance changes. Recent developments in image transformations such as neural style transfer have emerged as an alternative to address such appearance gaps. In this work, we propose to combine an image transformation network and a feature-learning network to improve long-term localization performance. Given night-to-day image pairs, the image transformation network transforms the night images into day-like conditions prior to feature matching; the feature network learns to detect keypoint locations with their associated descriptor values, which can be passed to a classical pose estimator to compute the relative poses. We conducted various experiments to examine the effectiveness of combining style transfer and feature learning and its training strategy, showing that such a combination greatly improves long-term localization performance.|
|**2023-06-22**|**GT-TSCH: Game-Theoretic Distributed TSCH Scheduler for Low-Power IoT Networks**|Omid Tavallaie et.al.|[2306.13039v1](http://arxiv.org/abs/2306.13039v1)|null|Time-Slotted Channel Hopping (TSCH) is a synchronous medium access mode of the IEEE 802.15.4e standard designed for providing low-latency and highly-reliable end-to-end communication. TSCH constructs a communication schedule by combining frequency channel hopping with Time Division Multiple Access (TDMA). In recent years, IETF designed several standards to define general mechanisms for the implementation of TSCH. However, the problem of updating the TSCH schedule according to the changes of the wireless link quality and node's traffic load left unresolved. In this paper, we use non-cooperative game theory to propose GT-TSCH, a distributed TSCH scheduler designed for low-power IoT applications. By considering selfish behavior of nodes in packet forwarding, GT-TSCH updates the TSCH schedule in a distributed approach with low control overhead by monitoring the queue length, the place of the node in the Directed Acyclic Graph (DAG) topology, the quality of the wireless link, and the data packet generation rate. We prove the existence and uniqueness of Nash equilibrium in our game model and we find the optimal number of TSCH Tx timeslots to update the TSCH slotframe. To examine the performance of our contribution, we implement GT-TSCH on Zolertia Firefly IoT motes and the Contiki-NG Operating System (OS). The evaluation results reveal that GT-TSCH improves performance in terms of throughput and end-to-end delay compared to the state-of-the-art method.|
|**2023-06-22**|**Impacts and Risk of Generative AI Technology on Cyber Defense**|Subash Neupane et.al.|[2306.13033v1](http://arxiv.org/abs/2306.13033v1)|null|Generative Artificial Intelligence (GenAI) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. With its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, GenAI has garnered significant attention and adoption. However, the increasing adoption of GenAI raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. To combat the threats posed by GenAI, we propose leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. This paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of GenAI techniques in each phase of the CKC framework. We also analyze the strategies employed by threat actors and examine their utilization throughout different phases of the CKC, highlighting the implications for cyber defense. Additionally, we propose GenAI-enabled defense strategies that are both attack-aware and adaptive. These strategies encompass various techniques such as detection, deception, and adversarial training, among others, aiming to effectively mitigate the risks posed by GenAI-induced cyber threats.|
|**2023-06-22**|**Methodological Reflections on the MOND/Dark Matter Debate**|Patrick M. Duerr et.al.|[2306.13026v1](http://arxiv.org/abs/2306.13026v1)|null|The paper re-examines the principal methodological questions, arising in the debate over the cosmological standard model's postulate of Dark Matter vs. rivalling proposals that modify standard (Newtonian and general-relativistic) gravitational theory, the so-called Modified Newtonian Dynamics (MOND) and its subsequent extensions. What to make of such seemingly radical challenges of cosmological orthodoxy? In the first part of our paper, we assess MONDian theories through the lens of key ideas of major 20th century philosophers of science (Popper, Kuhn, Lakatos, and Laudan), thereby rectifying widespread misconceptions and misapplications of these ideas common in the pertinent MOND-related literature. None of these classical methodological frameworks, which render precise and systematise the more intuitive judgements prevalent in the scientific community, yields a favourable verdict on MOND and its successors -- contrary to claims in the MOND-related literature by some of these theories' advocates; the respective theory appraisals are largely damning. Drawing on these insights, the paper's second part zooms in on the most common complaint about MONDian theories, their ad-hocness. We demonstrate how the recent coherentist model of ad-hocness captures, and fleshes out, the underlying -- but too often insufficiently articulated -- hunches underlying this critique. MONDian theories indeed come out as severely ad hoc: they do not cohere well with either theoretical or empirical-factual background knowledge. In fact, as our complementary comparison with the cosmological standard model's Dark Matter postulate shows, with respect to ad-hocness, MONDian theories fare worse than the cosmological standard model.|
|**2023-06-22**|**Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US**|Jonathan H. Rystrøm et.al.|[2306.13000v1](http://arxiv.org/abs/2306.13000v1)|null|As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we want generative models to play in society.|
|**2023-06-22**|**Can a single image processing algorithm work equally well across all phases of DCE-MRI?**|Adam G. Tattersall et.al.|[2306.12988v1](http://arxiv.org/abs/2306.12988v1)|null|Image segmentation and registration are said to be challenging when applied to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes rapid changes in intensity in the region of interest and elsewhere, which can lead to false positive predictions for segmentation tasks and confound the image registration similarity metric. While it is widely assumed that contrast changes increase the difficulty of these tasks, to our knowledge no work has quantified these effects. In this paper we examine the effect of training with different ratios of contrast enhanced (CE) data on two popular tasks: segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and VTN. We experimented further by strategically using the available datasets through pretraining and fine tuning with different splits of data. We found that to create a generalisable model, pretraining with CE data and fine tuning with non-CE data gave the best result. This interesting find could be expanded to other deep learning based image processing tasks with DCE-MRI and provide significant improvements to the models performance.|
|**2023-06-22**|**Towards More Realistic Membership Inference Attacks on Large Diffusion Models**|Jan Dubiński et.al.|[2306.12983v1](http://arxiv.org/abs/2306.12983v1)|null|Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.|
|**2023-06-22**|**On the Degree of Dynamical Packing in the Kepler Multi-planet Systems**|Alysa Obertas et.al.|[2306.12967v1](http://arxiv.org/abs/2306.12967v1)|null|Current planet formation theories rely on initially compact orbital configurations undergoing a (possibly extended) phase of giant impacts following the dispersal of the dissipative protoplanetary disk. The orbital architectures of observed mature exoplanet systems have likely been strongly sculpted by chaotic dynamics, instabilities, and giant impacts. One possible signature of systems continually reshaped by instabilities and mergers is their dynamical packing. Early Kepler data showed that many multi-planet systems are maximally packed - placing an additional planet between an observed pair would make the system unstable. However, this result relied on placing the inserted planet in the most optimistic configuration for stability (e.g., circular orbits). While this would be appropriate in an ordered and dissipative picture of planet formation (i.e. planets dampen into their most stable configurations), we argue that this best-case scenario for stability is rarely realized due to the strongly chaotic nature of planet formation. Consequently, the degree of dynamical packing in multi-planet systems under a realistic formation model is likely significantly higher than previously realized. We examine the full Kepler multi planet sample through this new lens, showing that ~60-95% of Kepler multi-planet systems are strongly packed and that dynamical packing increases with multiplicity. This may be a signature of dynamical sculpting or of undetected planets, showing that dynamical packing is an important metric that can be incorporated into planet formation modelling or when searching for unseen planets.|
|**2023-06-22**|**A New Constraint on the Simulation of the Intergalactic Medium through the Evolution of the Neutral Hydrogen Fraction in the Epoch of Reionization**|S. Mobina Hosseini et.al.|[2306.12954v1](http://arxiv.org/abs/2306.12954v1)|null|The thermal history of the intergalactic medium is full of extremely useful data in the field of astrophysics and cosmology. In other words, by examining this environment in different redshifts, the effects of cosmology and astrophysics can be observed side by side. Therefore, simulation is our very powerful tool to reach a suitable model for the intergalactic medium, both in terms of cosmology and astrophysics. In this work, we have simulated the intergalactic medium with the help of the 21cmFAST code and compared the evolution of the neutral hydrogen fraction in different initial conditions. Considerable works arbitrarily determine many important effective parameters in the thermal history of the intergalactic medium without any constraints, and usually, there is a lot of flexibility for modeling. Nonetheless, in this work, by focusing on the evolution of the neutral hydrogen fraction in different models and comparing it with observational data, we have eliminated many models and introduced only limited simulation models that could confirm the observations with sufficient accuracy. This issue becomes thoroughly vital from the point that, in addition to restricting the models through the neutral hydrogen fraction, it can also impose restrictions on the parameters affecting its changes. However, we hope that in future works, by enhancing the observational data and increasing their accuracy, more compatible models with the history of the intergalactic medium can be achieved.|
|**2023-06-22**|**The source of electrons at comet 67P**|P. Stephenson et.al.|[2306.12942v1](http://arxiv.org/abs/2306.12942v1)|null|We examine the origin of electrons in a weakly outgassing comet, using Rosetta mission data and a 3D collisional model of electrons at a comet. We have calculated a new dataset of electron-impact ionization (EII) frequency throughout the Rosetta escort phase, with measurements of the Rosetta Plasma Consortium's Ion and Electron Sensor (RPC/IES). The EII frequency is evaluated in 15-minute intervals and compared to other Rosetta datasets.   Electron-impact ionization is the dominant source of electrons at 67P away from perihelion and is highly variable (by up to three orders of magnitude). Around perihelion, EII is much less variable and less efficient than photoionization at Rosetta. Several drivers of the EII frequency are identified, including magnetic field strength and the outgassing rate. Energetic electrons are correlated to the Rosetta-upstream solar wind potential difference, confirming that the ionizing electrons are solar wind electrons accelerated by an ambipolar field.   The collisional test particle model incorporates a spherically symmetric, pure water coma and all the relevant electron-neutral collision processes. Electric and magnetic fields are stationary model inputs, and are computed using a fully-kinetic, collisionless Particle-in-Cell simulation. Collisional electrons are modelled at outgassing rates of $Q=10^{26}$ s$^{-1}$ and $Q=1.5\times10^{27}$ s$^{-1}$. Secondary electrons are the dominant population within a weakly outgassing comet. These are produced by collisions of solar wind electrons with the neutral coma.   The implications of large ion flow speed estimates at Rosetta, away from perihelion, are discussed in relation to multi-instrument studies and the new results of the EII frequency obtained in the present study.|
|**2023-06-22**|**Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation**|Ran Zhang et.al.|[2306.12916v1](http://arxiv.org/abs/2306.12916v1)|null|While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good quality outputs and as an evaluator correlates moderately with human evaluations though it is prone to giving lower scores. ChatGPT also seems to be very adept at normalizing historical text. We finally test ChatGPT in a scenario with adversarially attacked and unseen source documents and find that ChatGPT is better at omission and entity swap than negating against its prior knowledge.|
|**2023-06-22**|**Consistent maps and their associated representation theorems**|Charles L. Samuels et.al.|[2306.12887v1](http://arxiv.org/abs/2306.12887v1)|null|A 2009 article of Allcock and Vaaler examined the vector space $\mathcal G := \overline{\mathbb Q}^\times/\overline{\mathbb Q}^\times_{\mathrm{tors}}$ over $\mathbb Q$, describing its completion with respect to the Weil height as a certain $L^1$ space. By involving an object called a consistent map, the author began efforts to establish representation theorems for the duals of spaces related to $\mathcal G$. Specifically, we provided representation theorems for the algebraic and continuous duals of $\overline{\mathbb Q}^\times/{\overline{\mathbb Z}}^\times$. We explore further applications of consistent maps to provide representation theorems for duals of other spaces arising in the work of Allcock and Vaaler. In particular, we establish a connection between consistent maps and locally constant functions on the space of places of $\overline{\mathbb Q}$. We further apply our new results to recover, as a corollary, a main theorem of our previous work.|
|**2023-06-22**|**Estimating dynamic treatment regimes for ordinal outcomes with household interference: Application in household smoking cessation**|Cong Jiang et.al.|[2306.12865v1](http://arxiv.org/abs/2306.12865v1)|null|The focus of precision medicine is on decision support, often in the form of dynamic treatment regimes (DTRs), which are sequences of decision rules. At each decision point, the decision rules determine the next treatment according to the patient's baseline characteristics, the information on treatments and responses accrued by that point, and the patient's current health status, including symptom severity and other measures. However, DTR estimation with ordinal outcomes is rarely studied, and rarer still in the context of interference - where one patient's treatment may affect another's outcome. In this paper, we introduce the proposed weighted proportional odds model (WPOM): a regression-based, doubly-robust approach to single-stage DTR estimation for ordinal outcomes. This method also accounts for the possibility of interference between individuals sharing a household through the use of covariate balancing weights derived from joint propensity scores. Examining different types of balancing weights, we verify the double robustness of WPOM with our adjusted weights via simulation studies. We further extend WPOM to multi-stage DTR estimation with household interference. Lastly, we demonstrate our proposed methodology in the analysis of longitudinal survey data from the Population Assessment of Tobacco and Health study, which motivates this work.|
|**2023-06-22**|**Price elasticity of electricity demand: Using instrumental variable regressions to address endogeneity and autocorrelation of high-frequency time series**|Silvana Tiedemann et.al.|[2306.12863v1](http://arxiv.org/abs/2306.12863v1)|null|This paper examines empirical methods for estimating the response of aggregated electricity demand to high-frequency price signals, the short-term elasticity of electricity demand. We investigate how the endogeneity of prices and the autocorrelation of the time series, which are particularly pronounced at hourly granularity, affect and distort common estimators. After developing a controlled test environment with synthetic data that replicate key statistical properties of electricity demand, we show that not only the ordinary least square (OLS) estimator is inconsistent (due to simultaneity), but so is a regular instrumental variable (IV) regression (due to autocorrelation). Using wind as an instrument, as it is commonly done, may result in an estimate of the demand elasticity that is inflated by an order of magnitude. We visualize the reason for the Thams bias using causal graphs and show that its magnitude depends on the autocorrelation of both the instrument, and the dependent variable. We further incorporate and adapt two extensions of the IV estimation, conditional IV and nuisance IV, which have recently been proposed by Thams et al. (2022). We show that these extensions can identify the true short-term elasticity in a synthetic setting and are thus particularly promising for future empirical research in this field.|
|**2023-06-22**|**Accuracy evaluation of a Low-Cost Differential Global Positioning System for mobile robotics**|Christian Blesing et.al.|[2306.12826v1](http://arxiv.org/abs/2306.12826v1)|null|Differential GPS, commonly referred as DGPS, is a well-known and very accurate localization system for many outdoor applications in particular for mobile outdoor robotics. The most common drawback of DGPS systems are the high costs for both base station and receivers. In this paper, we present a setup that uses third-party open-source software and a Ublox ZED-F9P chip to build a ROS-enabled low-cost DGPS setup that is ready to use in a few hours. The main goal of this paper is to analyze and evaluate the repetitive and absolute accuracy of the system. The first measurement also examines the differences between a SAPOS base station and a locally installed one consisting of low-cost components. During the evaluation process of the absolute accuracy, a moving mobile robot is used on the receiver side. It is tracked through a highly accurate VICON motion capture system.|
|**2023-06-22**|**An Enhanced Massive Black Hole Occupation Fraction Predicted in Cluster Dwarf Galaxies**|Michael Tremmel et.al.|[2306.12813v1](http://arxiv.org/abs/2306.12813v1)|null|The occupation fraction of massive black holes (MBHs) in low mass galaxies offers interesting insights into initial black hole seeding mechanisms and their mass assembly history, though disentangling these two effects remains challenging. Using the Romulus cosmological simulations we examine the impact of environment on the occupation fraction of MBHs in low mass galaxies. Unlike most modern cosmological simulations, Romulus seeds MBHs based on local gas properties, selecting very dense, pristine, and rapidly collapsing regions in the early Universe as sites to host MBHs without assuming anything about MBH occupation as a function of galaxy stellar mass, or halo mass, a priori. The simulations predict that dwarf galaxies with M$_{\star}<10^9$ M$_{\odot}$ in cluster environments are approximately two times more likely to host a MBH compared to those in the field. The predicted occupation fractions are remarkably consistent with those of nuclear star clusters. Across cluster and field environments, dwarf galaxies with earlier formation times are more likely to host a MBH. Thus, while the MBH occupation function is similar between cluster and field environments at high redshift ($z>3$), a difference arises as late-forming dwarfs -- which do not exist in the cluster environment -- begin to dominate in the field and pull the MBH occupation fraction down for low mass galaxies. Additionally, prior to in-fall some cluster dwarfs are similar to progenitors of massive, isolated galaxies, indicating that they might have grown to higher masses had they not been impeded by the cluster environment. While the population of MBHs in dwarf galaxies is already widely understood to be important for understanding MBH formation, this work demonstrates that environmental dependence is important to consider as future observations search for low mass black holes in dwarf galaxies.|
|**2023-06-22**|**Searching for supermassive charged gravitinos in underground experiments**|Krzysztof A. Meissner et.al.|[2306.12797v1](http://arxiv.org/abs/2306.12797v1)|null|We examine possible experimental signatures that may be exploited to search for stable supermassive particles with electric charges of $O(1)$ in future underground experiments, and the upcoming JUNO experiment in particular. The telltale signal would be a correlated sequence of three or more nuclear recoils along a straight line, corresponding to the motion of a non-relativistic ($\beta <10^{-2}$) particle that could enter the detector from any direction. We provide some preliminary estimates for the expected event rates.|
|**2023-06-22**|**Russian assimilatory palatalization is incomplete neutralization**|Sejin Oh et.al.|[2306.12789v1](http://arxiv.org/abs/2306.12789v1)|null|Incomplete neutralization refers to phonetic traces of underlying contrasts in phonologically neutralizing contexts. The present study examines one such context: Russian assimilatory palatalization in C+j sequences. Russian contrasts plain and palatalized consonants, with the plain consonants having a secondary articulation involving retraction of the tongue dorsum (velarization/uvularization). However, Russian also has stop-glide sequences that form near-minimal pairs with palatalized stops. In the environment preceding palatal glides, the contrast between palatalized and plain consonants is neutralized, due to the palatalization of the plain stop (assimilatory palatalization). The purpose of the study is to explore whether the neutralization is complete. To do so, we conducted an electromagnetic articulography (EMA) experiment examining temporal coordination and the spatial position of the tongue body in underlyingly palatalized consonants and those derived from assimilatory palatalization. Articulatory results from four native speakers of Russian revealed that gestures in both conditions are coordinated as complex segments, i.e., they are palatalized consonants. However, there are differences across conditions consistent with the residual presence of a tongue dorsum retraction gesture in the plain obstruents. We conclude that neutralization of the plain-palatal contrast in Russian is incomplete; consonants in the assimilatory palatalization condition exhibit inter-gestural coordination characteristic of palatalized consonants along with residual evidence of an underlying tongue dorsum retraction (velarization/uvularization) gesture.|
|**2023-06-22**|**Arrangement of nearby minima and saddles in the mixed spherical energy landscapes**|Jaron Kent-Dobias et.al.|[2306.12779v1](http://arxiv.org/abs/2306.12779v1)|null|The mixed spherical models were recently found to violate long-held assumptions about mean-field glassy dynamics. In particular, the threshold energy, where most stationary points are marginal and which in the simpler pure models attracts long-time dynamics, seems to lose significance. Here, we compute the typical distribution of stationary points relative to each other in mixed models with a replica symmetric complexity. We examine the stability of nearby points, accounting for the presence of an isolated eigenvalue in their spectrum due to their proximity. Despite finding rich structure not present in the pure models, we find nothing that distinguishes the points that do attract the dynamics. Instead, we find new geometric significance of the old threshold energy, and invalidate pictures of the arrangement most of marginal inherent states into a continuous manifold.|
|**2023-06-22**|**Quark flavour physics: status and future prospects**|Vladimir V. Gligorov et.al.|[2306.12728v1](http://arxiv.org/abs/2306.12728v1)|null|Quark flavour physics is the study of hadrons, their properties, and their decays into other particles. As a discipline it simultaneously catalogues the nature of physical states within the Standard Model of particle physics, and in doing so tests the consistency and completeness of the Standard Model's description of reality. Following the discovery of the Higgs field, it is more essential than ever to critically examine the Standard Model's own coherence. Precision studies of quark flavour are one of the most sensitive experimental instruments for this task. I give a brief and necessarily selective overview of recent developments in quark flavour physics and discuss prospects for the next generation of experiments and facilities, with an emphasis on the energy scales of beyond Standard Model physics probed by these types of measurements.|
|**2023-06-22**|**Multilingual Neural Machine Translation System for Indic to Indic Languages**|Sudhansu Bala Das et.al.|[2306.12693v1](http://arxiv.org/abs/2306.12693v1)|null|This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs implemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All the models are evaluated using the BLEU score. In addition, the languages are classified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI). The effect of language relatedness on MNMT model efficiency is studied. Owing to the presence of large corpora from English (EN) to ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To achieve this, English- Indic (EN-IL) models are also developed, with and without the usage of related languages. Results reveal that using related languages is beneficial for the WI group only, while it is detrimental for the EI group and shows an inconclusive effect on the DR group, but it is useful for EN-IL models. Thus, related language groups are used to develop pivot MNMT models. Furthermore, the IL corpora are transliterated from the corresponding scripts to a modified ITRANS script, and the best MNMT models from the previous approaches are built on the transliterated corpus. It is observed that the usage of pivot models greatly improves MNMT baselines with AS-TA achieving the minimum BLEU score and PA-HI achieving the maximum score. Among languages, AS, ML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the best. Transliteration also helps the models with few exceptions. The best increment of scores is observed in ML, TA, and BN and the worst average increment is observed in KN, HI, and PA, across all languages. The best model obtained is the PA-HI language pair trained on PAWI transliterated corpus which gives 24.29 BLEU.|
|**2023-06-22**|**PCA matrix denoising is uniform**|Xin T. Tong et.al.|[2306.12690v1](http://arxiv.org/abs/2306.12690v1)|null|Principal component analysis (PCA) is a simple and popular tool for processing high-dimensional data. We investigate its effectiveness for matrix denoising. We assume i.i.d. high dimensional Gaussian noises with standard deviation $\sigma$ are added to clean data generated from a low dimensional subspace. We show that the distance between each pair of PCA-denoised data point and the clean data point is uniformly bounded by $\Otilde(\sigma)$, assuming a low-rank data matrix with mild singular value assumptions. We show such a condition could arise even if the data lies on curves. We then provide a general lower bound for the error of the denoised data matrix, which indicates PCA denoising gives a uniform error bound that is rate-optimal. Furthermore, we examine how the error bound impacts downstream applications such as empirical risk minimization, clustering, and manifold learning. Numerical results validate our theoretical findings and reveal the importance of the uniform error.|
|**2023-06-21**|**Social Media Emotions and IPO Returns**|Domonkos F. Vamossy et.al.|[2306.12602v1](http://arxiv.org/abs/2306.12602v1)|null|I examine potential mechanisms behind two stylized facts of initial public offerings (IPOs) returns. By analyzing investor sentiment expressed on StockTwits and Twitter, I find that emotions conveyed through these social media platforms can help explain the mispricing of IPO stocks. The abundance of information and opinions shared on social media can generate hype around certain stocks, leading to investors' irrational buying and selling decisions. This can result in an overvaluation of the stock in the short term but often leads to a correction in the long term as the stock's performance fails to meet the inflated expectations. In particular, I find that IPOs with high levels of pre-IPO investor enthusiasm tend to have a significantly higher first-day return of 29.54%, compared to IPOs with lower levels of pre-IPO investor enthusiasm, which have an average first-day return of 16.91%. However, this initial enthusiasm may be misplaced, as IPOs with high pre-IPO investor enthusiasm demonstrate a much lower average long-run industry-adjusted return of -8.53%, compared to IPOs with lower pre-IPO investor enthusiasm, which have an average long-run industry-adjusted return of -1.1%.|
|**2023-06-21**|**Investigating the accelerated expansion of the Universe through updated constraints on viable $f(R)$ models within the metric formalism**|Kumar Ravi et.al.|[2306.12585v1](http://arxiv.org/abs/2306.12585v1)|null|Modified theories of gravity encompass a class of $f(R)$-models that seek to elucidate the observed late time accelerated expansion of the universe. In this study, we examine a set of viable $f(R)$ models (Hu-Sawicki: two cases, Satrobinsky, Tsujikawa, exponential and arcTanh models) in metric formalism, using recent cosmological data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H\textsc{ii} starburst galaxies, and local measurements of the Hubble parameter $H_0$. The model parameters are constrained using a Bayesian analysis with the Monte Carlo Markov Chain method. We employ statistical tools such as the Akaike Information Criterion, Bayesian Information Criterion, and reduced chi-square statistics to conduct a comparative investigation of these models. We determine the transition redshift, the evolution of total equation-of-state (EoS) parameter, and the EoS for the component responsible for current accelerated expansion to characterize the expansion's evolution. Taking into account the ``Hubble tension," we perform the study with and without a Gaussian prior for $H_0$ from local measurements. Our findings are as follows: (i) in many cases the $f(R)$ models are strongly favored over the standard $\Lambda$CDM model, (ii) the deviation parameter ($b$) significantly deviates from zero in several cases, (iii) the inclusion of local $H_0$ not only increases the fitted value of $H_0$ (as expected) but also affects the gap between predictions of $f(R)$ models and the $\Lambda$CDM model, and (iv) the relevant quantities characterizing the (accelerated) expansion of the universe obtained in our models are consistent with those obtained in a model-independent way by others. Our investigation and results present a compelling case for pursuing further research on $f(R)$ models with future observations to come.|
|**2023-06-21**|**Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases**|Risako Ando et.al.|[2306.12567v1](http://arxiv.org/abs/2306.12567v1)|null|This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.|
|**2023-06-21**|**Global MHD Simulations of the Time-Dependent Corona**|Roberto Lionello et.al.|[2306.12551v1](http://arxiv.org/abs/2306.12551v1)|null|We describe, test, and apply a technique to incorporate full-sun, surface flux evolution into an MHD model of the global solar corona. Requiring only maps of the evolving surface flux, our method is similar to that of Lionello et al. (2013), but we introduce two ways to correct the electric field at the lower boundary to mitigate spurious currents. We verify the accuracy of our procedures by comparing to a reference simulation, driven with known flows and electric fields. We then present a thermodynamic MHD calculation lasting one solar rotation driven by maps from the magnetic flux evolution model of Schrijver & DeRosa (2003). The dynamic, time-dependent nature of the model corona is illustrated by examining the evolution of the open flux boundaries and forward modeled EUV emission, which evolve in response to surface flows and the emergence and cancellation flux. Although our main goal is to present the method, we briefly investigate the relevance of this evolution to properties of the slow solar wind, examining the mapping of dipped field lines to the topological signatures of the "S-Web" and comparing charge state ratios computed in the time-dependently driven run to a steady state equivalent. Interestingly, we find that driving on its own does not significantly improve the charge states ratios, at least in this modest resolution run that injects minimal helicity. Still, many aspects of the time-dependently driven model cannot be captured with traditional steady-state methods, and such a technique may be particularly relevant for the next generation of solar wind and CME models.|
|**2023-06-21**|**Bayesian inference and role of astrocytes in amyloid-beta dynamics with modelling of Alzheimer's disease using clinical data**|Hina Shaheen et.al.|[2306.12520v1](http://arxiv.org/abs/2306.12520v1)|null|Alzheimer's disease (AD) is a prominent, worldwide, age-related neurodegenerative disease that currently has no systemic treatment. Strong evidence suggests that permeable amyloid-beta peptide (Abeta) oligomers, astrogliosis and reactive astrocytosis cause neuronal damage in AD. A large amount of Abeta is secreted by astrocytes, which contributes to the total Abeta deposition in the brain. This suggests that astrocytes may also play a role in AD, leading to increased attention to their dynamics and associated mechanisms. Therefore, in the present study, we developed and evaluated novel stochastic models for Abeta growth using ADNI data to predict the effect of astrocytes on AD progression in a clinical trial. In the AD case, accurate prediction is required for a successful clinical treatment plan. Given that AD studies are observational in nature and involve routine patient visits, stochastic models provide a suitable framework for modelling AD. Using the approximate Bayesian computation (ABC) approach, the AD etiology may be modelled as a multi-state disease process. As a result, we use this approach to examine the weak and strong influence of astrocytes at multiple disease progression stages using ADNI data from the baseline to 2-year visits for AD patients whose ages ranged from 50 to 90 years. Based on ADNI data, we discovered that the strong astrocyte effect (i.e., a higher concentration of astrocytes as compared to Abeta) could help to lower or clear the growth of Abeta, which is a key to slowing down AD progression.|
